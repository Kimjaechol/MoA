import { NextRequest, NextResponse } from "next/server";

/**
 * POST /api/chat
 * Send a message and get an AI response.
 * Body: { user_id, session_id, content, channel?, category? }
 *
 * The `category` field enables category-aware skill routing:
 *   daily, work, document, coding, image, music, other
 *
 * Resilient design: works even without Supabase or API keys.
 * Supabase persistence is best-effort; AI responses always returned.
 */
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { user_id, session_id, content, channel = "web", category = "other", is_desktop = false } = body;

    if (!content || typeof content !== "string" || !content.trim()) {
      return NextResponse.json({ error: "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”." }, { status: 400 });
    }

    // Try to get Supabase client (non-blocking â€” works without it)
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    let supabase: any = null;
    try {
      const { getServiceSupabase } = await import("@/lib/supabase");
      supabase = getServiceSupabase();
    } catch {
      // Supabase not configured â€” continue without persistence
    }

    // 1. Save user message (best-effort, non-blocking)
    if (supabase && user_id && session_id) {
      try {
        await supabase.from("moa_chat_messages").insert({
          user_id, session_id, role: "user",
          content: content.trim(), channel, category,
        });
      } catch { /* persistence failure â€” non-fatal */ }
    }

    // 2. Check for local file access requests from non-desktop browser
    if (!is_desktop && /([A-Za-z]:\\|ë‚´\s*ì»´í“¨í„°|ë¡œì»¬\s*íŒŒì¼|E\s*ë“œë¼ì´ë¸Œ|C\s*ë“œë¼ì´ë¸Œ|D\s*ë“œë¼ì´ë¸Œ)/.test(content)) {
      return NextResponse.json({
        reply: "ë¡œì»¬ íŒŒì¼ì— ì ‘ê·¼í•˜ë ¤ë©´ MoA ë°ìŠ¤í¬í†± ì•±ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n" +
          "MoA ë°ìŠ¤í¬í†± ì•±ì„ ì„¤ì¹˜í•˜ë©´ Eë“œë¼ì´ë¸Œ ë“± ë¡œì»¬ íŒŒì¼ì„ ì§ì ‘ ê´€ë¦¬í•  ìˆ˜ ìˆì–´ìš”.\n\n" +
          "ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ì—ì„œ ì›í´ë¦­ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: /download",
        model: "local/system",
        category,
        credits_used: 0,
        timestamp: new Date().toISOString(),
      });
    }

    // 3. Generate AI response (category-aware, always succeeds)
    const aiResponse = await generateResponse(content.trim(), user_id, category, supabase);

    // 4. Deduct credits (best-effort, non-blocking)
    // Apply 2x multiplier when using MoA's server-level API keys
    let creditInfo: { balance?: number; cost?: number } = {};
    if (supabase && user_id) {
      try {
        creditInfo = await deductCredits(supabase, user_id, aiResponse.model, aiResponse.usedEnvKey);
      } catch { /* credit deduction failure â€” non-fatal */ }
    }

    // 5. Save AI response (best-effort, non-blocking)
    if (supabase && user_id && session_id) {
      try {
        await supabase.from("moa_chat_messages").insert({
          user_id, session_id, role: "assistant",
          content: aiResponse.text, channel,
          model_used: aiResponse.model, category,
        });
      } catch { /* persistence failure â€” non-fatal */ }
    }

    return NextResponse.json({
      reply: aiResponse.text,
      model: aiResponse.model,
      category,
      credits_used: creditInfo.cost ?? 0,
      credits_remaining: creditInfo.balance,
      key_source: aiResponse.usedEnvKey ? "moa" : "user",
      timestamp: new Date().toISOString(),
    });
  } catch {
    // Ultimate fallback â€” always return a response, never 500
    return NextResponse.json({
      reply: "ì•ˆë…•í•˜ì„¸ìš”! MoA AIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
      model: "local/fallback",
      category: "other",
      timestamp: new Date().toISOString(),
    });
  }
}

/**
 * GET /api/chat?user_id=xxx&session_id=yyy
 * Fetch chat history for a session.
 */
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const userId = searchParams.get("user_id");
    const sessionId = searchParams.get("session_id");
    const limit = parseInt(searchParams.get("limit") ?? "50", 10);

    if (!userId || !sessionId) {
      return NextResponse.json({ messages: [] });
    }

    let supabase;
    try {
      const { getServiceSupabase } = await import("@/lib/supabase");
      supabase = getServiceSupabase();
    } catch {
      return NextResponse.json({ messages: [] });
    }

    const { data, error } = await supabase
      .from("moa_chat_messages")
      .select("id, role, content, model_used, created_at")
      .eq("user_id", userId)
      .eq("session_id", sessionId)
      .order("created_at", { ascending: true })
      .limit(limit);

    if (error) {
      return NextResponse.json({ messages: [] });
    }

    return NextResponse.json({ messages: data ?? [] });
  } catch {
    return NextResponse.json({ messages: [] });
  }
}

/* -----------------------------------------------------------------
   Credit Deduction
   ----------------------------------------------------------------- */

const MODEL_CREDIT_COSTS: Record<string, number> = {
  "local/slm-default": 0, "local/fallback": 0,
  "groq/kimi-k2-0905": 1, "groq/llama-3.3-70b-versatile": 1,
  "gemini/gemini-2.5-flash": 2, "gemini/gemini-2.0-flash": 2,
  "deepseek/deepseek-chat": 3,
  "openai/gpt-4o": 5, "openai/gpt-4o-mini": 3,
  "anthropic/claude-sonnet-4-5": 8, "anthropic/claude-haiku-4-5": 4,
  "openai/gpt-5": 10,
  "anthropic/claude-opus-4-6": 15,
};

function getCreditCost(model: string): number {
  if (MODEL_CREDIT_COSTS[model] !== undefined) return MODEL_CREDIT_COSTS[model];
  if (model.startsWith("groq/")) return 1;
  if (model.startsWith("gemini/")) return 2;
  if (model.startsWith("deepseek/")) return 3;
  if (model.startsWith("openai/")) return 5;
  if (model.startsWith("anthropic/")) return 8;
  return 0;
}

/** MoA server key multiplier: 2x credit cost when users use MoA's API keys */
const ENV_KEY_MULTIPLIER = 2;

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function deductCredits(supabase: any, userId: string, model: string, usedEnvKey: boolean): Promise<{ balance: number; cost: number }> {
  const baseCost = getCreditCost(model);
  // Apply 2x multiplier when using MoA's server-level API keys
  const cost = usedEnvKey ? baseCost * ENV_KEY_MULTIPLIER : baseCost;
  if (cost === 0) return { balance: -1, cost: 0 };

  // Get or initialize credits
  let { data: credits } = await supabase
    .from("moa_credits")
    .select("balance, monthly_used")
    .eq("user_id", userId)
    .single();

  if (!credits) {
    await supabase.from("moa_credits").insert({
      user_id: userId, balance: 100, monthly_quota: 100, monthly_used: 0, plan: "free",
      quota_reset_at: new Date(Date.now() + 30 * 24 * 60 * 60 * 1000).toISOString(),
    });
    credits = { balance: 100, monthly_used: 0 };
  }

  // Allow usage even if balance is low (don't block chat)
  const newBalance = Math.max(0, credits.balance - cost);
  const newUsed = (credits.monthly_used ?? 0) + cost;

  await supabase
    .from("moa_credits")
    .update({ balance: newBalance, monthly_used: newUsed, updated_at: new Date().toISOString() })
    .eq("user_id", userId);

  const keyLabel = usedEnvKey ? " (MoA í‚¤ 2x)" : "";
  await supabase.from("moa_credit_transactions").insert({
    user_id: userId, amount: -cost, balance_after: newBalance,
    tx_type: "usage", description: `ì±„íŒ… - ${model}${keyLabel}`, model_used: model,
  });

  return { balance: newBalance, cost };
}

/* -----------------------------------------------------------------
   Category-Aware AI Response Generator
   ----------------------------------------------------------------- */

interface AIResponse {
  text: string;
  model: string;
  usedEnvKey: boolean;
}

/**
 * Strict language enforcement rule â€” appended to all system prompts.
 * Prevents CJK language mixing (e.g. Japanese in Korean responses).
 */
const LANGUAGE_RULE = `

[CRITICAL LANGUAGE RULE]
You MUST respond in the SAME language as the user's message.
- If the user writes in Korean, respond ONLY in Korean. Never mix Japanese, Chinese, or any other language.
- If the user writes in Japanese, respond ONLY in Japanese.
- If the user writes in English, respond ONLY in English.
- If the user writes in Chinese, respond ONLY in Chinese.
- If the user explicitly requests a different language (e.g. "ì˜ì–´ë¡œ ë‹µí•´ì¤˜"), follow that instruction.
- English technical terms (API, URL, code snippets) are acceptable in any language.
- ABSOLUTELY DO NOT mix different Asian languages. For example, never use Japanese words (ã‚ã‚Šã¾ã›ã‚“, ã¡ã‚‡ã£ã¨, etc.) in a Korean response. This is strictly forbidden.
`;

/** Category-specific system prompt prefixes for LLM routing */
const CATEGORY_SYSTEM_PROMPTS: Record<string, string> = {
  daily: `You are a daily life assistant. Help with schedules, weather, translations, lifestyle tips, and general questions.${LANGUAGE_RULE}`,
  work: `You are a professional work assistant. Help with emails, reports, meeting notes, data analysis, and business tasks.${LANGUAGE_RULE}`,
  document: `You are a document specialist. Help with document creation, summarization, conversion, synthesis, and formatting.${LANGUAGE_RULE}`,
  coding: `You are an expert software engineer. Help with code writing, debugging, code review, and automated coding tasks. Include code snippets and technical details.${LANGUAGE_RULE}`,
  image: `You are an image/visual AI assistant. Help with image generation prompts, editing instructions, image analysis, and style transfer.${LANGUAGE_RULE}`,
  music: `You are a music AI assistant. Help with composition, lyrics writing, TTS, and music analysis.${LANGUAGE_RULE}`,
  other: `You are MoA, a versatile AI assistant with 100+ skills across 15 channels. Help with any request.${LANGUAGE_RULE}`,
};

/** Category-specific skill sets for routing */
const CATEGORY_SKILLS: Record<string, string[]> = {
  daily: ["weather", "calendar", "translate", "search", "news", "maps"],
  work: ["email", "notion", "airtable", "slack", "github", "calendar", "summarize"],
  document: ["summarize", "editor", "synthesis", "convert", "pptx", "pdf"],
  coding: ["code", "debug", "github", "autocode", "vision", "terminal"],
  image: ["fal-ai", "replicate", "vision", "image-edit", "style-transfer"],
  music: ["tts", "suno", "lyrics", "music-analysis", "podcast"],
  other: ["search", "translate", "summarize", "general"],
};

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function generateResponse(message: string, userId: string, category: string, supabase: any): Promise<AIResponse> {
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  let activeKeys: any[] = [];
  let strategy = "cost-efficient";

  // Try to fetch user settings from Supabase (non-blocking)
  if (supabase && userId) {
    try {
      const { data: keys } = await supabase
        .from("moa_user_api_keys")
        .select("provider, encrypted_key, is_active")
        .eq("user_id", userId)
        .eq("is_active", true);
      activeKeys = keys ?? [];
    } catch { /* table may not exist yet */ }

    try {
      const { data: settings } = await supabase
        .from("moa_user_settings")
        .select("model_strategy")
        .eq("user_id", userId)
        .single();
      strategy = settings?.model_strategy ?? "cost-efficient";
    } catch { /* table may not exist yet */ }
  }

  // Try to call real LLM API (env keys or user keys)
  try {
    const llmResult = await tryLlmCall(message, category, strategy, activeKeys);
    if (llmResult) {
      return llmResult;
    }
  } catch { /* LLM call failed â€” fall through to smart response */ }

  // Fallback: always-available smart contextual response (no API key used)
  const modelUsed = selectModelName(strategy, activeKeys);
  const text = generateSmartResponse(message, category, modelUsed, "");
  return { text, model: modelUsed, usedEnvKey: false };
}

/**
 * Attempt real LLM API call.
 * Priority: user's own keys (1x credit) > MoA server keys (2x credit).
 * Returns usedEnvKey=true when MoA's server-level API key was used.
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function tryLlmCall(message: string, category: string, strategy: string, keys: any[]): Promise<AIResponse | null> {
  const systemPrompt = CATEGORY_SYSTEM_PROMPTS[category] ?? CATEGORY_SYSTEM_PROMPTS.other;
  const skills = CATEGORY_SKILLS[category] ?? CATEGORY_SKILLS.other;
  const enrichedSystem = `${systemPrompt}\n\nAvailable skills for this category: ${skills.join(", ")}`;

  // Server-level env keys (MoA-provided â†’ 2x credit)
  const envAnthropicKey = process.env.ANTHROPIC_API_KEY;
  const envOpenaiKey = process.env.OPENAI_API_KEY;
  const envGeminiKey = process.env.GEMINI_API_KEY;
  const envGroqKey = process.env.GROQ_API_KEY;
  const envDeepseekKey = process.env.DEEPSEEK_API_KEY;

  // User-provided keys (stored in DB â†’ 1x credit)
  const userAnthropicKey = keys.find((k: { provider: string }) => k.provider === "anthropic")?.encrypted_key;
  const userOpenaiKey = keys.find((k: { provider: string }) => k.provider === "openai")?.encrypted_key;
  const userGeminiKey = keys.find((k: { provider: string }) => k.provider === "gemini")?.encrypted_key;
  const userGroqKey = keys.find((k: { provider: string }) => k.provider === "groq")?.encrypted_key;
  const userDeepseekKey = keys.find((k: { provider: string }) => k.provider === "deepseek")?.encrypted_key;

  // Helper: pick user key first (1x), fallback to env key (2x)
  const pickKey = (userKey?: string, envKey?: string): { key: string; isEnv: boolean } | null => {
    if (userKey) return { key: userKey, isEnv: false };
    if (envKey) return { key: envKey, isEnv: true };
    return null;
  };

  // Max-performance: use the best model available
  if (strategy === "max-performance") {
    const anthropicInfo = pickKey(userAnthropicKey, envAnthropicKey);
    if (anthropicInfo) {
      const result = await callAnthropic(anthropicInfo.key, enrichedSystem, message, "claude-opus-4-6");
      if (result) return { text: result, model: "anthropic/claude-opus-4-6", usedEnvKey: anthropicInfo.isEnv };
    }
    const openaiInfo = pickKey(userOpenaiKey, envOpenaiKey);
    if (openaiInfo) {
      const result = await callOpenAI(openaiInfo.key, enrichedSystem, message, "gpt-5");
      if (result) return { text: result, model: "openai/gpt-5", usedEnvKey: openaiInfo.isEnv };
    }
  }

  // Cost-efficient: try cheaper models first
  const groqInfo = pickKey(userGroqKey, envGroqKey);
  if (groqInfo) {
    const result = await callGroq(groqInfo.key, enrichedSystem, message);
    if (result) return { text: result, model: "groq/llama-3.3-70b-versatile", usedEnvKey: groqInfo.isEnv };
  }

  const geminiInfo = pickKey(userGeminiKey, envGeminiKey);
  if (geminiInfo) {
    const result = await callGemini(geminiInfo.key, enrichedSystem, message);
    if (result) return { text: result, model: "gemini/gemini-2.5-flash", usedEnvKey: geminiInfo.isEnv };
  }

  const deepseekInfo = pickKey(userDeepseekKey, envDeepseekKey);
  if (deepseekInfo) {
    const result = await callDeepSeek(deepseekInfo.key, enrichedSystem, message);
    if (result) return { text: result, model: "deepseek/deepseek-chat", usedEnvKey: deepseekInfo.isEnv };
  }

  // Fallback: try remaining env keys for OpenAI/Anthropic in cost-efficient mode
  if (strategy !== "max-performance") {
    const openaiInfo = pickKey(userOpenaiKey, envOpenaiKey);
    if (openaiInfo) {
      const result = await callOpenAI(openaiInfo.key, enrichedSystem, message, "gpt-4o-mini");
      if (result) return { text: result, model: "openai/gpt-4o-mini", usedEnvKey: openaiInfo.isEnv };
    }
    const anthropicInfo = pickKey(userAnthropicKey, envAnthropicKey);
    if (anthropicInfo) {
      const result = await callAnthropic(anthropicInfo.key, enrichedSystem, message, "claude-haiku-4-5");
      if (result) return { text: result, model: "anthropic/claude-haiku-4-5", usedEnvKey: anthropicInfo.isEnv };
    }
  }

  return null;
}

async function callAnthropic(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: { "Content-Type": "application/json", "x-api-key": key, "anthropic-version": "2023-06-01" },
      body: JSON.stringify({ model, max_tokens: 4096, system, messages: [{ role: "user", content: message }] }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.content?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callOpenAI(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model, messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callGemini(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${key}`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ contents: [{ parts: [{ text: `${system}\n\n${message}` }] }], generationConfig: { maxOutputTokens: 4096 } }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.candidates?.[0]?.content?.parts?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callGroq(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "llama-3.3-70b-versatile", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callDeepSeek(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "deepseek-chat", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
function selectModelName(strategy: string, keys: any[]): string {
  if (strategy === "max-performance") {
    if (keys.some((k: { provider: string }) => k.provider === "anthropic")) return "anthropic/claude-opus-4-6";
    if (keys.some((k: { provider: string }) => k.provider === "openai")) return "openai/gpt-5";
  }
  if (keys.some((k: { provider: string }) => k.provider === "groq")) return "groq/llama-3.3-70b-versatile";
  if (keys.some((k: { provider: string }) => k.provider === "gemini")) return "gemini/gemini-2.5-flash";
  if (keys.some((k: { provider: string }) => k.provider === "deepseek")) return "deepseek/deepseek-chat";
  return "local/slm-default";
}

function generateSmartResponse(message: string, category: string, model: string, _prefix: string): string {
  const lowerMsg = message.toLowerCase();
  const catLabel = getCategoryLabel(category);
  const catInfo = CATEGORY_SKILLS[category]?.join(", ") ?? "general";

  // Greeting patterns (Korean + English)
  if (/^(ì•ˆë…•|hi|hello|í•˜ì´|ë°˜ê°€|í—¬ë¡œ|ã…ã…‡|moa|ëª¨ì•„)/.test(lowerMsg)) {
    return `ì•ˆë…•í•˜ì„¸ìš”! MoA AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œë¡œ ëŒ€í™” ì¤‘ì´ì—ìš”.\n\nğŸ’¡ ì´ëŸ° ê²ƒë“¤ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”:\n${getCategoryExamples(category)}\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?`;
  }

  // Help / capabilities
  if (/^(ë„ì›€|help|ë­ í•  ìˆ˜|ê¸°ëŠ¥|ìŠ¤í‚¬|í•  ìˆ˜ ìˆ)/.test(lowerMsg)) {
    return getCategoryHelp(category, "");
  }

  // Weather
  if (/ë‚ ì”¨|weather|ê¸°ì˜¨/.test(lowerMsg)) {
    return `ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.\n\nğŸŒ¤ï¸ **ì˜¤ëŠ˜ì˜ ë‚ ì”¨** (ì„œìš¸ ê¸°ì¤€)\n- í˜„ì¬ ê¸°ì˜¨: 3Â°C\n- ìµœê³ /ìµœì €: 7Â°C / -1Â°C\n- ìŠµë„: 45%\n- ë¯¸ì„¸ë¨¼ì§€: ë³´í†µ\n\nğŸ’¡ ë” ì •í™•í•œ ì‹¤ì‹œê°„ ë‚ ì”¨ë¥¼ ì›í•˜ì‹œë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.`;
  }

  // Model / strategy info
  if (/ì „ëµ|strategy|ëª¨ë¸|ê°€ì„±ë¹„|ìµœëŒ€ì„±ëŠ¥|api|í‚¤/.test(lowerMsg)) {
    return `í˜„ì¬ ëª¨ë¸ ì „ëµ ì •ë³´ì…ë‹ˆë‹¤:\n\nğŸ“Š **ê°€ì„±ë¹„ ì „ëµ** (ê¸°ë³¸)\nâ€¢ Groq (ë¬´ë£Œ) â†’ Gemini Flash â†’ DeepSeek â†’ í”„ë¦¬ë¯¸ì—„\n\nğŸ§  **ìµœê³ ì„±ëŠ¥ ì „ëµ**\nâ€¢ Claude Opus 4.6 â†’ GPT-5\n\ní˜„ì¬ ì‚¬ìš© ì¤‘: **${model}**\n\nğŸ’¡ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ì‹¤ì‹œê°„ AI ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!`;
  }

  // Channel / integration
  if (/ì±„ë„|channel|ì¹´ì¹´ì˜¤|í…”ë ˆê·¸ë¨|ë””ìŠ¤ì½”ë“œ|ìŠ¬ë™|ë¼ì¸/.test(lowerMsg)) {
    return `MoAëŠ” 15ê°œ ì±„ë„ì„ ì§€ì›í•©ë‹ˆë‹¤:\n\nğŸ“± **ë©”ì‹ ì €**: ì¹´ì¹´ì˜¤í†¡, í…”ë ˆê·¸ë¨, Discord, WhatsApp, LINE, Slack\nğŸŒ **ì›¹**: ì›¹ì±„íŒ… (ì§€ê¸ˆ ì‚¬ìš© ì¤‘)\nğŸ“§ **ê¸°íƒ€**: ì´ë©”ì¼, SMS ë“±\n\nì±„ë„ í—ˆë¸Œì—ì„œ ê° ì±„ë„ ì—°ë™ ì„¤ì •ì„ í•  ìˆ˜ ìˆì–´ìš”.`;
  }

  // Coding
  if (/ì½”ë“œ|ì½”ë”©|í”„ë¡œê·¸ë˜ë°|ê°œë°œ|debug|ë²„ê·¸/.test(lowerMsg)) {
    return `ë„¤, ì½”ë”© ì‘ì—…ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ’»\n\n> "${message}"\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œì…ë‹ˆë‹¤.\n\nğŸ”§ **ì½”ë”© ë„ì›€ ê¸°ëŠ¥:**\nâ€¢ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·°\nâ€¢ ë²„ê·¸ ë¶„ì„ ë° ë””ë²„ê¹…\nâ€¢ ìë™ì½”ë”© (/autocode)\nâ€¢ Vision ê¸°ë°˜ UI ê²€ì¦\n\nğŸ’¡ ë” ì •í™•í•œ ì½”ë”© ë„ì›€ì„ ìœ„í•´ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.`;
  }

  // Document
  if (/ë¬¸ì„œ|ë³´ê³ ì„œ|ìš”ì•½|ë²ˆì—­|ê¸€|ì‘ì„±/.test(lowerMsg)) {
    return `ë¬¸ì„œ ì‘ì—…ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ“„\n\n> "${message}"\n\n**ë¬¸ì„œ ê´€ë ¨ ê¸°ëŠ¥:**\nâ€¢ ğŸ“ ë¬¸ì„œ ì‘ì„± Â· ìš”ì•½ Â· ë²ˆì—­\nâ€¢ ğŸ“‘ ì¢…í•©ë¬¸ì„œ ì‘ì„± (/synthesis)\nâ€¢ ğŸ“Š PPTX í”„ë ˆì  í…Œì´ì…˜ ìƒì„±\nâ€¢ ğŸ“„ í˜•ì‹ ë³€í™˜ (DOCX, HWPX, PDF, XLSX)\nâ€¢ âœï¸ TipTap ì—ë””í„°\n\nì–´ë–¤ ë¬¸ì„œ ì‘ì—…ì„ ì§„í–‰í• ê¹Œìš”?`;
  }

  // Generic fallback â€” friendly, informative
  return `ë„¤, ë§ì”€ì„ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤! ğŸ˜Š\n\n> "${message}"\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œì—ì„œ ëŒ€í™” ì¤‘ì´ì—ìš”.\ní™œìš© ê°€ëŠ¥í•œ ìŠ¤í‚¬: ${catInfo}\n\nğŸ’¡ API í‚¤ê°€ ì„¤ì •ë˜ë©´ ì‹¤ì‹œê°„ AIê°€ ë” ì •í™•í•˜ê²Œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.\në§ˆì´í˜ì´ì§€ì—ì„œ Gemini, Groq, DeepSeek ë“±ì˜ ë¬´ë£Œ API í‚¤ë¥¼ ì„¤ì •í•´ë³´ì„¸ìš”!`;
}

function getCategoryExamples(category: string): string {
  const examples: Record<string, string> = {
    daily: "â€¢ ë‚ ì”¨ ì•Œë ¤ì¤˜\nâ€¢ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜\nâ€¢ ë§›ì§‘ ì¶”ì²œí•´ì¤˜\nâ€¢ ì¼ì • ì •ë¦¬í•´ì¤˜",
    work: "â€¢ ì´ë©”ì¼ ì´ˆì•ˆ ì‘ì„±í•´ì¤˜\nâ€¢ ë°ì´í„° ë¶„ì„ ë„ì™€ì¤˜\nâ€¢ íšŒì˜ë¡ ì •ë¦¬í•´ì¤˜\nâ€¢ ë³´ê³ ì„œ ì‘ì„±í•´ì¤˜",
    document: "â€¢ ë¬¸ì„œ ìš”ì•½í•´ì¤˜\nâ€¢ ì¢…í•©ë¬¸ì„œ ì‘ì„±í•´ì¤˜\nâ€¢ PPTXë¡œ ë³€í™˜í•´ì¤˜\nâ€¢ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì¤˜",
    coding: "â€¢ ì½”ë“œ ì‘ì„±í•´ì¤˜\nâ€¢ ë²„ê·¸ ì°¾ì•„ì¤˜\nâ€¢ ì½”ë“œ ë¦¬ë·°í•´ì¤˜\nâ€¢ ìë™ì½”ë”© ì‹œì‘í•´ì¤˜",
    image: "â€¢ ì´ë¯¸ì§€ ìƒì„±í•´ì¤˜\nâ€¢ ì´ë¯¸ì§€ ë¶„ì„í•´ì¤˜\nâ€¢ ìŠ¤íƒ€ì¼ ë³€í™˜í•´ì¤˜\nâ€¢ ì´ë¯¸ì§€ í¸ì§‘í•´ì¤˜",
    music: "â€¢ ì‘ê³¡í•´ì¤˜\nâ€¢ ê°€ì‚¬ ì‘ì„±í•´ì¤˜\nâ€¢ ì´ ê³¡ ë¶„ì„í•´ì¤˜\nâ€¢ TTS ë³€í™˜í•´ì¤˜",
    other: "â€¢ ë­˜ í•  ìˆ˜ ìˆì–´?\nâ€¢ ì±„ë„ ì•ˆë‚´í•´ì¤˜\nâ€¢ ëª¨ë¸ ì „ëµ ì•Œë ¤ì¤˜\nâ€¢ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”",
  };
  return examples[category] ?? examples.other;
}

function getCategoryLabel(category: string): string {
  const labels: Record<string, string> = {
    daily: "ì¼ìƒë¹„ì„œ", work: "ì—…ë¬´ë³´ì¡°", document: "ë¬¸ì„œì‘ì—…",
    coding: "ì½”ë”©ì‘ì—…", image: "ì´ë¯¸ì§€ì‘ì—…", music: "ìŒì•…ì‘ì—…", other: "ê¸°íƒ€",
  };
  return labels[category] ?? "ê¸°íƒ€";
}

function getCategoryHelp(category: string, prefix: string): string {
  const helps: Record<string, string> = {
    daily: `${prefix}**ì¼ìƒë¹„ì„œ** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸŒ¤ï¸ ë‚ ì”¨ ì¡°íšŒ Â· ğŸ“… ì¼ì • ê´€ë¦¬ Â· ğŸŒ ë²ˆì—­ Â· ğŸ” ì›¹ ê²€ìƒ‰\nğŸ“° ë‰´ìŠ¤ Â· ğŸ—ºï¸ ë§›ì§‘/ì¥ì†Œ ê²€ìƒ‰ Â· â° ì•ŒëŒ Â· ğŸ’¡ ìƒí™œ íŒ`,
    work: `${prefix}**ì—…ë¬´ë³´ì¡°** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“§ ì´ë©”ì¼ ì‘ì„± Â· ğŸ“Š ë°ì´í„° ë¶„ì„ Â· ğŸ“ íšŒì˜ë¡ ì •ë¦¬\nğŸ“ˆ ë³´ê³ ì„œ ì‘ì„± Â· ğŸ“‹ Notion/Airtable ì—°ë™ Â· ğŸ’¬ Slack ì—°ë™`,
    document: `${prefix}**ë¬¸ì„œì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“‹ ë¬¸ì„œ ìš”ì•½ Â· ğŸ“‘ ì¢…í•©ë¬¸ì„œ ì‘ì„± Â· ğŸ“„ í˜•ì‹ ë³€í™˜ (DOCX/HWPX/XLSX/PDF)\nğŸ¯ PPTX ìƒì„± Â· âœï¸ TipTap ì—ë””í„° Â· ğŸ”„ OCR ë³€í™˜`,
    coding: `${prefix}**ì½”ë”©ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ”§ ì½”ë“œ ì‘ì„± Â· ğŸ› ë””ë²„ê¹… Â· ğŸ“– ì½”ë“œ ë¦¬ë·°\nğŸ”„ ìë™ì½”ë”© (ì—ëŸ¬ ìë™ ìˆ˜ì •) Â· ğŸ–¥ï¸ Vision ê¸°ë°˜ UI ê²€ì¦ Â· ğŸ“¦ GitHub ì—°ë™`,
    image: `${prefix}**ì´ë¯¸ì§€ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ–¼ï¸ AI ì´ë¯¸ì§€ ìƒì„± (FAL AI) Â· âœ‚ï¸ ì´ë¯¸ì§€ í¸ì§‘\nğŸ” ì´ë¯¸ì§€ ë¶„ì„ (Vision) Â· ğŸ­ ìŠ¤íƒ€ì¼ ë³€í™˜ Â· ğŸ“ ë¦¬ì‚¬ì´ì¦ˆ/í¬ë§· ë³€í™˜`,
    music: `${prefix}**ìŒì•…ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ¼ AI ì‘ê³¡ Â· ğŸ¤ ê°€ì‚¬ ì‘ì„± Â· ğŸ”Š TTS ìŒì„± í•©ì„±\nğŸ¹ ìŒì•… ë¶„ì„ Â· ğŸ™ï¸ íŒŸìºìŠ¤íŠ¸ ìƒì„±`,
    other: `${prefix}**MoA**ê°€ ì§€ì›í•˜ëŠ” ì£¼ìš” ê¸°ëŠ¥:\n\nğŸ” ê²€ìƒ‰ Â· ğŸ“‹ ë¬¸ì„œ Â· ğŸ¨ ì´ë¯¸ì§€ Â· ğŸ’» ì½”ë”© Â· ğŸµ ìŒì•…\nğŸ“¡ 15ê°œ ì±„ë„ ì—°ë™ Â· 100+ ì „ë¬¸ ìŠ¤í‚¬ Â· ë‹¤ì¤‘ LLM ì§€ì›`,
  };
  return helps[category] ?? helps.other;
}
