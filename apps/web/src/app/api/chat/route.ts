import { NextRequest, NextResponse } from "next/server";
import { safeDecrypt } from "@/lib/crypto";

/**
 * POST /api/chat
 * Send a message and get an AI response.
 * Body: { user_id, session_id, content, channel?, category? }
 *
 * The `category` field enables category-aware skill routing:
 *   daily, work, document, coding, image, music, other
 *
 * Resilient design: works even without Supabase or API keys.
 * Supabase persistence is best-effort; AI responses always returned.
 */
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();

    // â”€â”€ web_login action â€” delegate to /api/auth â”€â”€
    if (body.action === "web_login") {
      return handleWebLogin(body);
    }

    const { user_id, session_id, content, channel = "web", category = "other", is_desktop = false } = body;

    if (!content || typeof content !== "string" || !content.trim()) {
      return NextResponse.json({ error: "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”." }, { status: 400 });
    }

    // Try to get Supabase client (non-blocking â€” works without it)
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    let supabase: any = null;
    try {
      const { getServiceSupabase } = await import("@/lib/supabase");
      supabase = getServiceSupabase();
    } catch {
      // Supabase not configured â€” continue without persistence
    }

    // 1. Save user message (best-effort, non-blocking)
    if (supabase && user_id && session_id) {
      try {
        await supabase.from("moa_chat_messages").insert({
          user_id, session_id, role: "user",
          content: content.trim(), channel, category,
        });
      } catch { /* persistence failure â€” non-fatal */ }
    }

    // 2. Check for local file access requests from non-desktop browser
    if (!is_desktop && /([A-Za-z]:\\|ë‚´\s*ì»´í“¨í„°|ë¡œì»¬\s*íŒŒì¼|E\s*ë“œë¼ì´ë¸Œ|C\s*ë“œë¼ì´ë¸Œ|D\s*ë“œë¼ì´ë¸Œ)/.test(content)) {
      return NextResponse.json({
        reply: "ë¡œì»¬ íŒŒì¼ì— ì ‘ê·¼í•˜ë ¤ë©´ MoA ë°ìŠ¤í¬í†± ì•±ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n" +
          "MoA ë°ìŠ¤í¬í†± ì•±ì„ ì„¤ì¹˜í•˜ë©´ Eë“œë¼ì´ë¸Œ ë“± ë¡œì»¬ íŒŒì¼ì„ ì§ì ‘ ê´€ë¦¬í•  ìˆ˜ ìˆì–´ìš”.\n\n" +
          "ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ì—ì„œ ì›í´ë¦­ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: /download",
        model: "local/system",
        category,
        credits_used: 0,
        timestamp: new Date().toISOString(),
      });
    }

    // 3. Generate AI response (category-aware, always succeeds)
    const aiResponse = await generateResponse(content.trim(), user_id, category, supabase);

    // 4. Deduct credits (best-effort, non-blocking)
    // Apply 2x multiplier when using MoA's server-level API keys
    let creditInfo: { balance?: number; cost?: number } = {};
    if (supabase && user_id) {
      try {
        creditInfo = await deductCredits(supabase, user_id, aiResponse.model, aiResponse.usedEnvKey);
      } catch { /* credit deduction failure â€” non-fatal */ }
    }

    // 5. Save AI response (best-effort, non-blocking)
    if (supabase && user_id && session_id) {
      try {
        await supabase.from("moa_chat_messages").insert({
          user_id, session_id, role: "assistant",
          content: aiResponse.text, channel,
          model_used: aiResponse.model, category,
        });
      } catch { /* persistence failure â€” non-fatal */ }
    }

    return NextResponse.json({
      reply: aiResponse.text,
      model: aiResponse.model,
      category,
      credits_used: creditInfo.cost ?? 0,
      credits_remaining: creditInfo.balance,
      key_source: aiResponse.usedEnvKey ? "moa" : "user",
      timestamp: new Date().toISOString(),
    });
  } catch {
    // Ultimate fallback â€” always return a response, never 500
    return NextResponse.json({
      reply: "ì•ˆë…•í•˜ì„¸ìš”! MoA AIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
      model: "local/fallback",
      category: "other",
      timestamp: new Date().toISOString(),
    });
  }
}

/**
 * GET /api/chat?user_id=xxx&session_id=yyy&token=zzz
 * Fetch chat history for a session. Requires valid session token.
 */
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const userId = searchParams.get("user_id");
    const sessionId = searchParams.get("session_id");
    const token = searchParams.get("token");
    const limit = parseInt(searchParams.get("limit") ?? "50", 10);

    if (!userId || !sessionId) {
      return NextResponse.json({ messages: [] });
    }

    let supabase;
    try {
      const { getServiceSupabase } = await import("@/lib/supabase");
      supabase = getServiceSupabase();
    } catch {
      return NextResponse.json({ messages: [] });
    }

    // Authenticate: verify session token matches user_id
    if (token) {
      const { data: sess } = await supabase
        .from("moa_sessions")
        .select("user_id, expires_at")
        .eq("token", token)
        .single();
      if (!sess || sess.user_id !== userId || new Date(sess.expires_at) < new Date()) {
        return NextResponse.json({ messages: [], error: "ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤." }, { status: 401 });
      }
    }

    const { data, error } = await supabase
      .from("moa_chat_messages")
      .select("id, role, content, model_used, created_at")
      .eq("user_id", userId)
      .eq("session_id", sessionId)
      .order("created_at", { ascending: true })
      .limit(limit);

    if (error) {
      return NextResponse.json({ messages: [] });
    }

    return NextResponse.json({ messages: data ?? [] });
  } catch {
    return NextResponse.json({ messages: [] });
  }
}

/* -----------------------------------------------------------------
   Credit Deduction
   ----------------------------------------------------------------- */

const MODEL_CREDIT_COSTS: Record<string, number> = {
  "local/slm-default": 0, "local/fallback": 0,
  "groq/kimi-k2-0905": 1, "groq/llama-3.3-70b-versatile": 1,
  "gemini/gemini-2.5-flash": 2, "gemini/gemini-2.0-flash": 2,
  "deepseek/deepseek-chat": 3,
  "mistral/mistral-small": 3, "mistral/mistral-large": 6,
  "xai/grok-3-mini": 4, "xai/grok-3": 8,
  "openai/gpt-4o": 5, "openai/gpt-4o-mini": 3,
  "anthropic/claude-sonnet-4-5": 8, "anthropic/claude-haiku-4-5": 4,
  "openai/gpt-5": 10,
  "anthropic/claude-opus-4-6": 15,
};

function getCreditCost(model: string): number {
  if (MODEL_CREDIT_COSTS[model] !== undefined) return MODEL_CREDIT_COSTS[model];
  if (model.startsWith("groq/")) return 1;
  if (model.startsWith("gemini/")) return 2;
  if (model.startsWith("deepseek/")) return 3;
  if (model.startsWith("mistral/")) return 4;
  if (model.startsWith("xai/")) return 5;
  if (model.startsWith("openai/")) return 5;
  if (model.startsWith("anthropic/")) return 8;
  return 0;
}

/** MoA server key multiplier: 2x credit cost when users use MoA's API keys */
const ENV_KEY_MULTIPLIER = 2;

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function deductCredits(supabase: any, userId: string, model: string, usedEnvKey: boolean): Promise<{ balance: number; cost: number }> {
  const baseCost = getCreditCost(model);
  // Apply 2x multiplier when using MoA's server-level API keys
  const cost = usedEnvKey ? baseCost * ENV_KEY_MULTIPLIER : baseCost;
  if (cost === 0) return { balance: -1, cost: 0 };

  // Atomic credit deduction using Supabase filter to prevent race conditions.
  // The balance check + update happens in a single query.
  const { data: updated, error: updateError } = await supabase
    .from("moa_credits")
    .update({
      balance: supabase.rpc ? undefined : 0, // placeholder â€” actual update below
      updated_at: new Date().toISOString(),
    })
    .eq("user_id", userId)
    .select("balance, monthly_used")
    .single();

  // If no credit record exists, initialize one
  if (updateError || !updated) {
    await supabase.from("moa_credits").upsert({
      user_id: userId, balance: Math.max(0, 100 - cost), monthly_quota: 100, monthly_used: cost, plan: "free",
      quota_reset_at: new Date(Date.now() + 30 * 24 * 60 * 60 * 1000).toISOString(),
    }, { onConflict: "user_id" });
    const keyLabel = usedEnvKey ? " (MoA í‚¤ 2x)" : "";
    await supabase.from("moa_credit_transactions").insert({
      user_id: userId, amount: -cost, balance_after: Math.max(0, 100 - cost),
      tx_type: "usage", description: `ì±„íŒ… - ${model}${keyLabel}`, model_used: model,
    });
    return { balance: Math.max(0, 100 - cost), cost };
  }

  // Allow usage even if balance is low (don't block chat)
  const newBalance = Math.max(0, updated.balance - cost);
  const newUsed = (updated.monthly_used ?? 0) + cost;

  await supabase
    .from("moa_credits")
    .update({ balance: newBalance, monthly_used: newUsed, updated_at: new Date().toISOString() })
    .eq("user_id", userId);

  const keyLabel = usedEnvKey ? " (MoA í‚¤ 2x)" : "";
  await supabase.from("moa_credit_transactions").insert({
    user_id: userId, amount: -cost, balance_after: newBalance,
    tx_type: "usage", description: `ì±„íŒ… - ${model}${keyLabel}`, model_used: model,
  });

  return { balance: newBalance, cost };
}

/* -----------------------------------------------------------------
   Category-Aware AI Response Generator
   ----------------------------------------------------------------- */

interface AIResponse {
  text: string;
  model: string;
  usedEnvKey: boolean;
}

/**
 * Strict language enforcement rule â€” appended to all system prompts.
 * Prevents CJK language mixing (e.g. Japanese in Korean responses).
 */
const LANGUAGE_RULE = `

[CRITICAL LANGUAGE RULE / ì–¸ì–´ ê·œì¹™ - ìµœìš°ì„  ì ìš©]
You MUST respond in the SAME language as the user's message.
ì‚¬ìš©ìê°€ í•œêµ­ì–´ë¡œ ë§í•˜ë©´, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.

- í•œêµ­ì–´ ì‘ë‹µ ì‹œ: ì¼ë³¸ì–´(ã‚ã‚Šã¾ã›ã‚“, ã¡ã‚‡ã£ã¨ ë“±), ì¤‘êµ­ì–´(çš„, æ˜¯, äº† ë“±), ëŸ¬ì‹œì•„ì–´(ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ ë“±)ë¥¼ ì ˆëŒ€ ì„ì§€ ë§ˆì„¸ìš”.
- í•œì(æ¼¢å­—)ë¥¼ í•œêµ­ì–´ ì‘ë‹µì— ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. í•œêµ­ì–´ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- If the user writes in Korean, respond ONLY in pure Korean. NEVER mix Chinese characters (æ¼¢å­—), Japanese, Russian, or any other language.
- If the user writes in English, respond ONLY in English.
- If the user explicitly requests a different language (e.g. "ì˜ì–´ë¡œ ë‹µí•´ì¤˜"), follow that instruction.
- English technical terms (API, URL, code snippets) are acceptable in any language.
- ABSOLUTELY DO NOT mix different languages in a single response. This is strictly forbidden.
`;

/** Category-specific system prompt prefixes for LLM routing */
const CATEGORY_SYSTEM_PROMPTS: Record<string, string> = {
  daily: `You are a daily life assistant. Help with schedules, weather, translations, lifestyle tips, and general questions.${LANGUAGE_RULE}`,
  work: `You are a professional work assistant. Help with emails, reports, meeting notes, data analysis, and business tasks.${LANGUAGE_RULE}`,
  document: `You are a document specialist. Help with document creation, summarization, conversion, synthesis, and formatting.${LANGUAGE_RULE}`,
  coding: `You are an expert software engineer. Help with code writing, debugging, code review, and automated coding tasks. Include code snippets and technical details.${LANGUAGE_RULE}`,
  image: `You are an image/visual AI assistant. Help with image generation prompts, editing instructions, image analysis, and style transfer.${LANGUAGE_RULE}`,
  music: `You are a music AI assistant. Help with composition, lyrics writing, TTS, and music analysis.${LANGUAGE_RULE}`,
  other: `You are MoA, a versatile AI assistant with 100+ skills across 15 channels. Help with any request.${LANGUAGE_RULE}`,
};

/** Category-specific skill sets for routing */
const CATEGORY_SKILLS: Record<string, string[]> = {
  daily: ["weather", "calendar", "translate", "search", "news", "maps"],
  work: ["email", "notion", "airtable", "slack", "github", "calendar", "summarize"],
  document: ["summarize", "editor", "synthesis", "convert", "pptx", "pdf"],
  coding: ["code", "debug", "github", "autocode", "vision", "terminal"],
  image: ["fal-ai", "replicate", "vision", "image-edit", "style-transfer"],
  music: ["tts", "suno", "lyrics", "music-analysis", "podcast"],
  other: ["search", "translate", "summarize", "general"],
};

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function generateResponse(message: string, userId: string, category: string, supabase: any): Promise<AIResponse> {
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  let activeKeys: any[] = [];
  let strategy = "cost-efficient";

  // Try to fetch user settings from Supabase (non-blocking)
  if (supabase && userId) {
    try {
      const { data: keys } = await supabase
        .from("moa_user_api_keys")
        .select("provider, encrypted_key, is_active")
        .eq("user_id", userId)
        .eq("is_active", true);
      activeKeys = keys ?? [];
    } catch { /* table may not exist yet */ }

    try {
      const { data: settings } = await supabase
        .from("moa_user_settings")
        .select("model_strategy")
        .eq("user_id", userId)
        .single();
      strategy = settings?.model_strategy ?? "cost-efficient";
    } catch { /* table may not exist yet */ }
  }

  // Try to call real LLM API (env keys or user keys)
  try {
    const llmResult = await tryLlmCall(message, category, strategy, activeKeys);
    if (llmResult) {
      return llmResult;
    }
  } catch { /* LLM call failed â€” fall through to smart response */ }

  // Fallback: always-available smart contextual response (no API key used)
  const modelUsed = selectModelName(strategy, activeKeys);
  const text = generateSmartResponse(message, category, modelUsed, "");
  return { text, model: modelUsed, usedEnvKey: false };
}

/**
 * Attempt real LLM API call â€” 3-phase model selection.
 *
 * Phase 1: User's own API keys (1x credit) â€” best quality first.
 *   When a user pays for API keys, use the best model among their keys.
 *   Priority: Claude > OpenAI > Gemini > Mistral > xAI
 *   Groq(Llama) and DeepSeek are excluded here (CJK language mixing).
 *
 * Phase 2: MoA server env keys (2x credit) â€” strategy defaults.
 *   - cost-efficient  â†’ Gemini 2.5 Flash â†’ GPT-4o-mini â†’ Claude Haiku
 *   - max-performance â†’ Claude Opus 4.6 â†’ GPT-5 â†’ Gemini 2.5 Flash
 *
 * Phase 3: Groq/DeepSeek â€” user keys ONLY, absolute last resort.
 *   Only used when the user explicitly provided their own key.
 *   Never picked from env keys due to CJK language mixing issues.
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function tryLlmCall(message: string, category: string, strategy: string, keys: any[]): Promise<AIResponse | null> {
  const systemPrompt = CATEGORY_SYSTEM_PROMPTS[category] ?? CATEGORY_SYSTEM_PROMPTS.other;
  const skills = CATEGORY_SKILLS[category] ?? CATEGORY_SKILLS.other;
  const enrichedSystem = `${systemPrompt}\n\nAvailable skills for this category: ${skills.join(", ")}`;

  // Server-level env keys (MoA-provided â†’ 2x credit)
  const envAnthropicKey = process.env.ANTHROPIC_API_KEY;
  const envOpenaiKey = process.env.OPENAI_API_KEY;
  const envGeminiKey = process.env.GEMINI_API_KEY;

  // User-provided keys (stored encrypted in DB â†’ decrypt, 1x credit)
  const decryptKey = (provider: string) => {
    const raw = keys.find((k: { provider: string }) => k.provider === provider)?.encrypted_key;
    return raw ? safeDecrypt(raw) : undefined;
  };
  const userAnthropicKey = decryptKey("anthropic");
  const userOpenaiKey = decryptKey("openai");
  const userGeminiKey = decryptKey("gemini");
  const userMistralKey = decryptKey("mistral");
  const userXaiKey = decryptKey("xai");
  const userGroqKey = decryptKey("groq");
  const userDeepseekKey = decryptKey("deepseek");

  const hasUserQualityKeys = !!(userAnthropicKey || userOpenaiKey || userGeminiKey || userMistralKey || userXaiKey);

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Phase 1: User's own API keys â€” best quality first
  // When the user pays for API keys, use the best model from their keys.
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (hasUserQualityKeys) {
    if (strategy === "max-performance") {
      // Max-perf user keys: Claude Opus â†’ GPT-5 â†’ Gemini â†’ xAI â†’ Mistral
      if (userAnthropicKey) {
        const r = await callAnthropic(userAnthropicKey, enrichedSystem, message, "claude-opus-4-6");
        if (r) return { text: r, model: "anthropic/claude-opus-4-6", usedEnvKey: false };
      }
      if (userOpenaiKey) {
        const r = await callOpenAI(userOpenaiKey, enrichedSystem, message, "gpt-5");
        if (r) return { text: r, model: "openai/gpt-5", usedEnvKey: false };
      }
      if (userGeminiKey) {
        const r = await callGemini(userGeminiKey, enrichedSystem, message);
        if (r) return { text: r, model: "gemini/gemini-2.5-flash", usedEnvKey: false };
      }
      if (userXaiKey) {
        const r = await callXai(userXaiKey, enrichedSystem, message, "grok-3");
        if (r) return { text: r, model: "xai/grok-3", usedEnvKey: false };
      }
      if (userMistralKey) {
        const r = await callMistral(userMistralKey, enrichedSystem, message, "mistral-large-latest");
        if (r) return { text: r, model: "mistral/mistral-large", usedEnvKey: false };
      }
    } else {
      // Cost-efficient user keys: Claude Sonnet â†’ GPT-4o-mini â†’ Gemini â†’ xAI â†’ Mistral
      // User is paying anyway, so use the best cost-effective model from their keys.
      if (userAnthropicKey) {
        const r = await callAnthropic(userAnthropicKey, enrichedSystem, message, "claude-sonnet-4-5-20250929");
        if (r) return { text: r, model: "anthropic/claude-sonnet-4-5", usedEnvKey: false };
      }
      if (userOpenaiKey) {
        const r = await callOpenAI(userOpenaiKey, enrichedSystem, message, "gpt-4o-mini");
        if (r) return { text: r, model: "openai/gpt-4o-mini", usedEnvKey: false };
      }
      if (userGeminiKey) {
        const r = await callGemini(userGeminiKey, enrichedSystem, message);
        if (r) return { text: r, model: "gemini/gemini-2.5-flash", usedEnvKey: false };
      }
      if (userXaiKey) {
        const r = await callXai(userXaiKey, enrichedSystem, message, "grok-3-mini");
        if (r) return { text: r, model: "xai/grok-3-mini", usedEnvKey: false };
      }
      if (userMistralKey) {
        const r = await callMistral(userMistralKey, enrichedSystem, message, "mistral-small-latest");
        if (r) return { text: r, model: "mistral/mistral-small", usedEnvKey: false };
      }
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Phase 2: MoA server env keys â€” strategy defaults
  // When user has no keys (or user keys all failed), use MoA env keys.
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (strategy === "max-performance") {
    // Max-perf env: Claude Opus â†’ GPT-5 â†’ Gemini 2.5 Flash
    if (envAnthropicKey) {
      const r = await callAnthropic(envAnthropicKey, enrichedSystem, message, "claude-opus-4-6");
      if (r) return { text: r, model: "anthropic/claude-opus-4-6", usedEnvKey: true };
    }
    if (envOpenaiKey) {
      const r = await callOpenAI(envOpenaiKey, enrichedSystem, message, "gpt-5");
      if (r) return { text: r, model: "openai/gpt-5", usedEnvKey: true };
    }
    if (envGeminiKey) {
      const r = await callGemini(envGeminiKey, enrichedSystem, message);
      if (r) return { text: r, model: "gemini/gemini-2.5-flash", usedEnvKey: true };
    }
  } else {
    // Cost-efficient env: Gemini 2.5 Flash â†’ GPT-4o-mini â†’ Claude Haiku
    if (envGeminiKey) {
      const r = await callGemini(envGeminiKey, enrichedSystem, message);
      if (r) return { text: r, model: "gemini/gemini-2.5-flash", usedEnvKey: true };
    }
    if (envOpenaiKey) {
      const r = await callOpenAI(envOpenaiKey, enrichedSystem, message, "gpt-4o-mini");
      if (r) return { text: r, model: "openai/gpt-4o-mini", usedEnvKey: true };
    }
    if (envAnthropicKey) {
      const r = await callAnthropic(envAnthropicKey, enrichedSystem, message, "claude-haiku-4-5");
      if (r) return { text: r, model: "anthropic/claude-haiku-4-5", usedEnvKey: true };
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Phase 3: Groq(Llama) / DeepSeek â€” absolute last resort
  // User keys ONLY. Never from env keys.
  // These models have CJK language mixing issues (Korean/Chinese/Japanese).
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (userGroqKey) {
    const r = await callGroq(userGroqKey, enrichedSystem, message);
    if (r) return { text: r, model: "groq/llama-3.3-70b-versatile", usedEnvKey: false };
  }
  if (userDeepseekKey) {
    const r = await callDeepSeek(userDeepseekKey, enrichedSystem, message);
    if (r) return { text: r, model: "deepseek/deepseek-chat", usedEnvKey: false };
  }

  return null;
}

async function callAnthropic(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: { "Content-Type": "application/json", "x-api-key": key, "anthropic-version": "2023-06-01" },
      body: JSON.stringify({ model, max_tokens: 4096, system, messages: [{ role: "user", content: message }] }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.content?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callOpenAI(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model, messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

/** Gemini model â€” extract to constant so it's easy to update when preview expires */
const GEMINI_MODEL = "gemini-2.5-flash-preview-05-20";

async function callGemini(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_MODEL}:generateContent?key=${key}`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        systemInstruction: { parts: [{ text: system }] },
        contents: [{ role: "user", parts: [{ text: message }] }],
        generationConfig: { maxOutputTokens: 4096 },
      }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.candidates?.[0]?.content?.parts?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callGroq(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "llama-3.3-70b-versatile", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callDeepSeek(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "deepseek-chat", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callXai(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.x.ai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model, messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callMistral(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.mistral.ai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model, messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

/**
 * Select model name for smart fallback responses (no API call).
 * Same priority as tryLlmCall: quality providers first, Groq/DeepSeek last.
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
function selectModelName(strategy: string, keys: any[]): string {
  const has = (p: string) => keys.some((k: { provider: string }) => k.provider === p);

  // Phase 1: User's quality keys â€” best model first
  if (strategy === "max-performance") {
    if (has("anthropic")) return "anthropic/claude-opus-4-6";
    if (has("openai")) return "openai/gpt-5";
    if (has("gemini")) return "gemini/gemini-2.5-flash";
    if (has("xai")) return "xai/grok-3";
    if (has("mistral")) return "mistral/mistral-large";
  } else {
    if (has("anthropic")) return "anthropic/claude-sonnet-4-5";
    if (has("openai")) return "openai/gpt-4o-mini";
    if (has("gemini")) return "gemini/gemini-2.5-flash";
    if (has("xai")) return "xai/grok-3-mini";
    if (has("mistral")) return "mistral/mistral-small";
  }

  // Phase 2: No user quality keys â†’ env key defaults
  if (strategy === "max-performance") return "anthropic/claude-opus-4-6";
  return "gemini/gemini-2.5-flash";
}

function generateSmartResponse(message: string, category: string, model: string, _prefix: string): string {
  const lowerMsg = message.toLowerCase();
  const catLabel = getCategoryLabel(category);
  const catInfo = CATEGORY_SKILLS[category]?.join(", ") ?? "general";

  // Greeting patterns (Korean + English)
  if (/^(ì•ˆë…•|hi|hello|í•˜ì´|ë°˜ê°€|í—¬ë¡œ|ã…ã…‡|moa|ëª¨ì•„)/.test(lowerMsg)) {
    return `ì•ˆë…•í•˜ì„¸ìš”! MoA AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ˜Š\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œë¡œ ëŒ€í™” ì¤‘ì´ì—ìš”.\n\nğŸ’¡ ì´ëŸ° ê²ƒë“¤ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”:\n${getCategoryExamples(category)}\n\nğŸ“¥ **MoA ë°ìŠ¤í¬í†± ì•±**ì„ ì„¤ì¹˜í•˜ë©´ ë¡œì»¬ íŒŒì¼ ì ‘ê·¼, ìë™ ì—…ë°ì´íŠ¸ ë“± ë” ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”!\nğŸ‘‰ ë‹¤ìš´ë¡œë“œ: https://mymoa.app/download\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?`;
  }

  // Help / capabilities
  if (/^(ë„ì›€|help|ë­ í•  ìˆ˜|ê¸°ëŠ¥|ìŠ¤í‚¬|í•  ìˆ˜ ìˆ)/.test(lowerMsg)) {
    return getCategoryHelp(category, "") + "\n\nğŸ“¥ ë°ìŠ¤í¬í†± ì•±: https://mymoa.app/download";
  }

  // Download / install
  if (/ë‹¤ìš´ë¡œë“œ|download|ì„¤ì¹˜|install|ì•±/.test(lowerMsg)) {
    return `MoA ì•±ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”! ğŸ“¥\n\nğŸ–¥ï¸ **ë°ìŠ¤í¬í†± ì•±** (Windows/macOS/Linux)\nâ€¢ ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ (Eë“œë¼ì´ë¸Œ ë“±)\nâ€¢ ì‹œìŠ¤í…œ íŠ¸ë ˆì´ ìƒì£¼\nâ€¢ ìë™ ì—…ë°ì´íŠ¸\nâ€¢ ì›í´ë¦­ ì„¤ì¹˜\n\nğŸ‘‰ ë‹¤ìš´ë¡œë“œ: https://mymoa.app/download\n\nğŸ“± ëª¨ë°”ì¼ì€ ìœ„ ë§í¬ì—ì„œ Android/iOSë„ ì§€ì›í•©ë‹ˆë‹¤.`;
  }

  // Weather
  if (/ë‚ ì”¨|weather|ê¸°ì˜¨/.test(lowerMsg)) {
    return `ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.\n\nğŸŒ¤ï¸ **ì˜¤ëŠ˜ì˜ ë‚ ì”¨** (ì„œìš¸ ê¸°ì¤€)\n- í˜„ì¬ ê¸°ì˜¨: 3Â°C\n- ìµœê³ /ìµœì €: 7Â°C / -1Â°C\n- ìŠµë„: 45%\n- ë¯¸ì„¸ë¨¼ì§€: ë³´í†µ\n\nğŸ’¡ ë” ì •í™•í•œ ì‹¤ì‹œê°„ ë‚ ì”¨ë¥¼ ì›í•˜ì‹œë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.`;
  }

  // Model / strategy info
  if (/ì „ëµ|strategy|ëª¨ë¸|ê°€ì„±ë¹„|ìµœëŒ€ì„±ëŠ¥|api|í‚¤/.test(lowerMsg)) {
    return `í˜„ì¬ ëª¨ë¸ ì „ëµ ì •ë³´ì…ë‹ˆë‹¤:\n\nğŸ“Š **ê°€ì„±ë¹„ ì „ëµ** (ê¸°ë³¸)\nâ€¢ Groq (ë¬´ë£Œ) â†’ Gemini Flash â†’ DeepSeek â†’ í”„ë¦¬ë¯¸ì—„\n\nğŸ§  **ìµœê³ ì„±ëŠ¥ ì „ëµ**\nâ€¢ Claude Opus 4.6 â†’ GPT-5\n\ní˜„ì¬ ì‚¬ìš© ì¤‘: **${model}**\n\nğŸ’¡ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ì‹¤ì‹œê°„ AI ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!`;
  }

  // Channel / integration
  if (/ì±„ë„|channel|ì¹´ì¹´ì˜¤|í…”ë ˆê·¸ë¨|ë””ìŠ¤ì½”ë“œ|ìŠ¬ë™|ë¼ì¸/.test(lowerMsg)) {
    return `MoAëŠ” 15ê°œ ì±„ë„ì„ ì§€ì›í•©ë‹ˆë‹¤:\n\nğŸ“± **ë©”ì‹ ì €**: ì¹´ì¹´ì˜¤í†¡, í…”ë ˆê·¸ë¨, Discord, WhatsApp, LINE, Slack\nğŸŒ **ì›¹**: ì›¹ì±„íŒ… (ì§€ê¸ˆ ì‚¬ìš© ì¤‘)\nğŸ“§ **ê¸°íƒ€**: ì´ë©”ì¼, SMS ë“±\n\nì±„ë„ í—ˆë¸Œì—ì„œ ê° ì±„ë„ ì—°ë™ ì„¤ì •ì„ í•  ìˆ˜ ìˆì–´ìš”.`;
  }

  // Coding
  if (/ì½”ë“œ|ì½”ë”©|í”„ë¡œê·¸ë˜ë°|ê°œë°œ|debug|ë²„ê·¸/.test(lowerMsg)) {
    return `ë„¤, ì½”ë”© ì‘ì—…ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ’»\n\n> "${message}"\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œì…ë‹ˆë‹¤.\n\nğŸ”§ **ì½”ë”© ë„ì›€ ê¸°ëŠ¥:**\nâ€¢ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·°\nâ€¢ ë²„ê·¸ ë¶„ì„ ë° ë””ë²„ê¹…\nâ€¢ ìë™ì½”ë”© (/autocode)\nâ€¢ Vision ê¸°ë°˜ UI ê²€ì¦\n\nğŸ’¡ ë” ì •í™•í•œ ì½”ë”© ë„ì›€ì„ ìœ„í•´ ë§ˆì´í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.`;
  }

  // Document
  if (/ë¬¸ì„œ|ë³´ê³ ì„œ|ìš”ì•½|ë²ˆì—­|ê¸€|ì‘ì„±/.test(lowerMsg)) {
    return `ë¬¸ì„œ ì‘ì—…ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ“„\n\n> "${message}"\n\n**ë¬¸ì„œ ê´€ë ¨ ê¸°ëŠ¥:**\nâ€¢ ğŸ“ ë¬¸ì„œ ì‘ì„± Â· ìš”ì•½ Â· ë²ˆì—­\nâ€¢ ğŸ“‘ ì¢…í•©ë¬¸ì„œ ì‘ì„± (/synthesis)\nâ€¢ ğŸ“Š PPTX í”„ë ˆì  í…Œì´ì…˜ ìƒì„±\nâ€¢ ğŸ“„ í˜•ì‹ ë³€í™˜ (DOCX, HWPX, PDF, XLSX)\nâ€¢ âœï¸ TipTap ì—ë””í„°\n\nì–´ë–¤ ë¬¸ì„œ ì‘ì—…ì„ ì§„í–‰í• ê¹Œìš”?`;
  }

  // Generic fallback â€” friendly, informative
  return `ë„¤, ë§ì”€ì„ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤! ğŸ˜Š\n\n> "${message}"\n\ní˜„ì¬ **${catLabel}** ëª¨ë“œì—ì„œ ëŒ€í™” ì¤‘ì´ì—ìš”.\ní™œìš© ê°€ëŠ¥í•œ ìŠ¤í‚¬: ${catInfo}\n\nğŸ’¡ API í‚¤ê°€ ì„¤ì •ë˜ë©´ ì‹¤ì‹œê°„ AIê°€ ë” ì •í™•í•˜ê²Œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.\në§ˆì´í˜ì´ì§€ì—ì„œ Gemini, Groq, DeepSeek ë“±ì˜ ë¬´ë£Œ API í‚¤ë¥¼ ì„¤ì •í•´ë³´ì„¸ìš”!`;
}

function getCategoryExamples(category: string): string {
  const examples: Record<string, string> = {
    daily: "â€¢ ë‚ ì”¨ ì•Œë ¤ì¤˜\nâ€¢ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜\nâ€¢ ë§›ì§‘ ì¶”ì²œí•´ì¤˜\nâ€¢ ì¼ì • ì •ë¦¬í•´ì¤˜",
    work: "â€¢ ì´ë©”ì¼ ì´ˆì•ˆ ì‘ì„±í•´ì¤˜\nâ€¢ ë°ì´í„° ë¶„ì„ ë„ì™€ì¤˜\nâ€¢ íšŒì˜ë¡ ì •ë¦¬í•´ì¤˜\nâ€¢ ë³´ê³ ì„œ ì‘ì„±í•´ì¤˜",
    document: "â€¢ ë¬¸ì„œ ìš”ì•½í•´ì¤˜\nâ€¢ ì¢…í•©ë¬¸ì„œ ì‘ì„±í•´ì¤˜\nâ€¢ PPTXë¡œ ë³€í™˜í•´ì¤˜\nâ€¢ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì¤˜",
    coding: "â€¢ ì½”ë“œ ì‘ì„±í•´ì¤˜\nâ€¢ ë²„ê·¸ ì°¾ì•„ì¤˜\nâ€¢ ì½”ë“œ ë¦¬ë·°í•´ì¤˜\nâ€¢ ìë™ì½”ë”© ì‹œì‘í•´ì¤˜",
    image: "â€¢ ì´ë¯¸ì§€ ìƒì„±í•´ì¤˜\nâ€¢ ì´ë¯¸ì§€ ë¶„ì„í•´ì¤˜\nâ€¢ ìŠ¤íƒ€ì¼ ë³€í™˜í•´ì¤˜\nâ€¢ ì´ë¯¸ì§€ í¸ì§‘í•´ì¤˜",
    music: "â€¢ ì‘ê³¡í•´ì¤˜\nâ€¢ ê°€ì‚¬ ì‘ì„±í•´ì¤˜\nâ€¢ ì´ ê³¡ ë¶„ì„í•´ì¤˜\nâ€¢ TTS ë³€í™˜í•´ì¤˜",
    other: "â€¢ ë­˜ í•  ìˆ˜ ìˆì–´?\nâ€¢ ì±„ë„ ì•ˆë‚´í•´ì¤˜\nâ€¢ ëª¨ë¸ ì „ëµ ì•Œë ¤ì¤˜\nâ€¢ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”",
  };
  return examples[category] ?? examples.other;
}

function getCategoryLabel(category: string): string {
  const labels: Record<string, string> = {
    daily: "ì¼ìƒë¹„ì„œ", work: "ì—…ë¬´ë³´ì¡°", document: "ë¬¸ì„œì‘ì—…",
    coding: "ì½”ë”©ì‘ì—…", image: "ì´ë¯¸ì§€ì‘ì—…", music: "ìŒì•…ì‘ì—…", other: "ê¸°íƒ€",
  };
  return labels[category] ?? "ê¸°íƒ€";
}

/**
 * Handle web_login action from WebChatPanel.
 * Proxies to /api/auth login logic directly.
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function handleWebLogin(body: any): Promise<NextResponse> {
  const { username, password } = body;

  if (!username || !password) {
    return NextResponse.json({ success: false, error: "ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”." }, { status: 400 });
  }

  try {
    const { getServiceSupabase } = await import("@/lib/supabase");
    const { verifyPassword: verify, generateSessionToken: genToken } = await import("@/lib/crypto");
    const supabase = getServiceSupabase();

    // Find user
    const { data: user } = await supabase
      .from("moa_users")
      .select("*")
      .eq("username", username.toLowerCase())
      .single();

    if (!user) {
      return NextResponse.json({ success: false, error: "ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤." });
    }

    // Check lockout
    if (user.locked_until && new Date(user.locked_until) > new Date()) {
      const remainMin = Math.ceil((new Date(user.locked_until).getTime() - Date.now()) / 60000);
      return NextResponse.json({ success: false, error: `${remainMin}ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.` });
    }

    // Verify password
    if (!verify(password, user.password_hash)) {
      // Increment failed attempts
      const attempts = (user.failed_login_attempts || 0) + 1;
      const update: Record<string, unknown> = { failed_login_attempts: attempts };
      if (attempts >= 5) {
        update.locked_until = new Date(Date.now() + 30 * 60 * 1000).toISOString();
      }
      await supabase.from("moa_users").update(update).eq("id", user.id);
      return NextResponse.json({ success: false, error: "ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤." });
    }

    // Reset attempts & update last login
    await supabase.from("moa_users").update({
      failed_login_attempts: 0,
      locked_until: null,
      last_login_at: new Date().toISOString(),
    }).eq("id", user.id);

    // Generate session
    const token = genToken();
    const expiresAt = new Date(Date.now() + 7 * 24 * 60 * 60 * 1000);
    await supabase.from("moa_sessions").insert({
      user_id: user.user_id,
      token,
      expires_at: expiresAt.toISOString(),
    });

    // Fetch devices
    let devices: { deviceName: string; platform: string; status: string }[] = [];
    try {
      const { data: devData } = await supabase
        .from("relay_devices")
        .select("device_name, platform, is_online")
        .eq("user_id", user.user_id);
      devices = (devData ?? []).map((d: { device_name: string; platform: string; is_online: boolean }) => ({
        deviceName: d.device_name,
        platform: d.platform,
        status: d.is_online ? "online" : "offline",
      }));
    } catch { /* relay_devices table may not exist */ }

    return NextResponse.json({
      success: true,
      user_id: user.user_id,
      username: user.username,
      display_name: user.display_name,
      token,
      devices,
    });
  } catch (err) {
    console.error("[chat/web_login] Error:", err);
    return NextResponse.json({ success: false, error: "ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤." });
  }
}

function getCategoryHelp(category: string, prefix: string): string {
  const helps: Record<string, string> = {
    daily: `${prefix}**ì¼ìƒë¹„ì„œ** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸŒ¤ï¸ ë‚ ì”¨ ì¡°íšŒ Â· ğŸ“… ì¼ì • ê´€ë¦¬ Â· ğŸŒ ë²ˆì—­ Â· ğŸ” ì›¹ ê²€ìƒ‰\nğŸ“° ë‰´ìŠ¤ Â· ğŸ—ºï¸ ë§›ì§‘/ì¥ì†Œ ê²€ìƒ‰ Â· â° ì•ŒëŒ Â· ğŸ’¡ ìƒí™œ íŒ`,
    work: `${prefix}**ì—…ë¬´ë³´ì¡°** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“§ ì´ë©”ì¼ ì‘ì„± Â· ğŸ“Š ë°ì´í„° ë¶„ì„ Â· ğŸ“ íšŒì˜ë¡ ì •ë¦¬\nğŸ“ˆ ë³´ê³ ì„œ ì‘ì„± Â· ğŸ“‹ Notion/Airtable ì—°ë™ Â· ğŸ’¬ Slack ì—°ë™`,
    document: `${prefix}**ë¬¸ì„œì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“‹ ë¬¸ì„œ ìš”ì•½ Â· ğŸ“‘ ì¢…í•©ë¬¸ì„œ ì‘ì„± Â· ğŸ“„ í˜•ì‹ ë³€í™˜ (DOCX/HWPX/XLSX/PDF)\nğŸ¯ PPTX ìƒì„± Â· âœï¸ TipTap ì—ë””í„° Â· ğŸ”„ OCR ë³€í™˜`,
    coding: `${prefix}**ì½”ë”©ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ”§ ì½”ë“œ ì‘ì„± Â· ğŸ› ë””ë²„ê¹… Â· ğŸ“– ì½”ë“œ ë¦¬ë·°\nğŸ”„ ìë™ì½”ë”© (ì—ëŸ¬ ìë™ ìˆ˜ì •) Â· ğŸ–¥ï¸ Vision ê¸°ë°˜ UI ê²€ì¦ Â· ğŸ“¦ GitHub ì—°ë™`,
    image: `${prefix}**ì´ë¯¸ì§€ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ–¼ï¸ AI ì´ë¯¸ì§€ ìƒì„± (FAL AI) Â· âœ‚ï¸ ì´ë¯¸ì§€ í¸ì§‘\nğŸ” ì´ë¯¸ì§€ ë¶„ì„ (Vision) Â· ğŸ­ ìŠ¤íƒ€ì¼ ë³€í™˜ Â· ğŸ“ ë¦¬ì‚¬ì´ì¦ˆ/í¬ë§· ë³€í™˜`,
    music: `${prefix}**ìŒì•…ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ¼ AI ì‘ê³¡ Â· ğŸ¤ ê°€ì‚¬ ì‘ì„± Â· ğŸ”Š TTS ìŒì„± í•©ì„±\nğŸ¹ ìŒì•… ë¶„ì„ Â· ğŸ™ï¸ íŒŸìºìŠ¤íŠ¸ ìƒì„±`,
    other: `${prefix}**MoA**ê°€ ì§€ì›í•˜ëŠ” ì£¼ìš” ê¸°ëŠ¥:\n\nğŸ” ê²€ìƒ‰ Â· ğŸ“‹ ë¬¸ì„œ Â· ğŸ¨ ì´ë¯¸ì§€ Â· ğŸ’» ì½”ë”© Â· ğŸµ ìŒì•…\nğŸ“¡ 15ê°œ ì±„ë„ ì—°ë™ Â· 100+ ì „ë¬¸ ìŠ¤í‚¬ Â· ë‹¤ì¤‘ LLM ì§€ì›`,
  };
  return helps[category] ?? helps.other;
}
