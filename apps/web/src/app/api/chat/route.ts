import { NextRequest, NextResponse } from "next/server";
import { getServiceSupabase } from "@/lib/supabase";

/**
 * POST /api/chat
 * Send a message and get an AI response.
 * Body: { user_id, session_id, content, channel?, category? }
 *
 * The `category` field enables category-aware skill routing:
 *   daily, work, document, coding, image, music, other
 */
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { user_id, session_id, content, channel = "web", category = "other" } = body;

    if (!user_id || !session_id) {
      return NextResponse.json({ error: "user_id and session_id are required" }, { status: 400 });
    }
    if (!content || typeof content !== "string" || !content.trim()) {
      return NextResponse.json({ error: "Message content is required" }, { status: 400 });
    }

    const supabase = getServiceSupabase();

    // 1. Save user message (with category)
    const { error: saveError } = await supabase.from("moa_chat_messages").insert({
      user_id,
      session_id,
      role: "user",
      content: content.trim(),
      channel,
      category,
    });

    if (saveError) {
      return NextResponse.json({ error: "Failed to save message" }, { status: 500 });
    }

    // 2. Generate AI response (category-aware)
    const aiResponse = await generateResponse(content.trim(), user_id, category, supabase);

    // 3. Save AI response
    const { error: aiSaveError } = await supabase.from("moa_chat_messages").insert({
      user_id,
      session_id,
      role: "assistant",
      content: aiResponse.text,
      channel,
      model_used: aiResponse.model,
      category,
    });

    if (aiSaveError) {
      return NextResponse.json({ error: "Failed to save AI response" }, { status: 500 });
    }

    return NextResponse.json({
      reply: aiResponse.text,
      model: aiResponse.model,
      category,
      timestamp: new Date().toISOString(),
    });
  } catch {
    return NextResponse.json({ error: "Internal server error" }, { status: 500 });
  }
}

/**
 * GET /api/chat?user_id=xxx&session_id=yyy
 * Fetch chat history for a session.
 */
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const userId = searchParams.get("user_id");
    const sessionId = searchParams.get("session_id");
    const limit = parseInt(searchParams.get("limit") ?? "50", 10);

    if (!userId || !sessionId) {
      return NextResponse.json({ error: "user_id and session_id required" }, { status: 400 });
    }

    const supabase = getServiceSupabase();

    const { data, error } = await supabase
      .from("moa_chat_messages")
      .select("id, role, content, model_used, created_at")
      .eq("user_id", userId)
      .eq("session_id", sessionId)
      .order("created_at", { ascending: true })
      .limit(limit);

    if (error) {
      return NextResponse.json({ error: "Failed to fetch messages" }, { status: 500 });
    }

    return NextResponse.json({ messages: data ?? [] });
  } catch {
    return NextResponse.json({ error: "Internal server error" }, { status: 500 });
  }
}

/* -----------------------------------------------------------------
   Category-Aware AI Response Generator
   ----------------------------------------------------------------- */

interface AIResponse {
  text: string;
  model: string;
}

/** Category-specific system prompt prefixes for LLM routing */
const CATEGORY_SYSTEM_PROMPTS: Record<string, string> = {
  daily: "You are a daily life assistant. Help with schedules, weather, translations, lifestyle tips, and general questions. Respond naturally in Korean.",
  work: "You are a professional work assistant. Help with emails, reports, meeting notes, data analysis, and business tasks. Respond in a professional Korean tone.",
  document: "You are a document specialist. Help with document creation, summarization, conversion, synthesis, and formatting. Respond in Korean.",
  coding: "You are an expert software engineer. Help with code writing, debugging, code review, and automated coding tasks. Include code snippets and technical details.",
  image: "You are an image/visual AI assistant. Help with image generation prompts, editing instructions, image analysis, and style transfer. Respond in Korean.",
  music: "You are a music AI assistant. Help with composition, lyrics writing, TTS, and music analysis. Respond in Korean.",
  other: "You are MoA, a versatile AI assistant with 100+ skills across 15 channels. Help with any request. Respond in Korean.",
};

/** Category-specific skill sets for routing */
const CATEGORY_SKILLS: Record<string, string[]> = {
  daily: ["weather", "calendar", "translate", "search", "news", "maps"],
  work: ["email", "notion", "airtable", "slack", "github", "calendar", "summarize"],
  document: ["summarize", "editor", "synthesis", "convert", "pptx", "pdf"],
  coding: ["code", "debug", "github", "autocode", "vision", "terminal"],
  image: ["fal-ai", "replicate", "vision", "image-edit", "style-transfer"],
  music: ["tts", "suno", "lyrics", "music-analysis", "podcast"],
  other: ["search", "translate", "summarize", "general"],
};

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function generateResponse(message: string, userId: string, category: string, supabase: any): Promise<AIResponse> {
  // Check if user has API keys configured
  const { data: keys } = await supabase
    .from("moa_user_api_keys")
    .select("provider, encrypted_key, is_active")
    .eq("user_id", userId)
    .eq("is_active", true);

  const { data: settings } = await supabase
    .from("moa_user_settings")
    .select("model_strategy")
    .eq("user_id", userId)
    .single();

  const strategy = settings?.model_strategy ?? "cost-efficient";
  const activeKeys = keys ?? [];

  // Try to call real LLM API if user has keys
  const llmResult = await tryLlmCall(message, category, strategy, activeKeys);
  if (llmResult) {
    return llmResult;
  }

  // Fallback: smart contextual response
  const modelUsed = selectModelName(strategy, activeKeys);
  const prefix = activeKeys.length === 0 ? "[ë¬´ë£Œ SLM] " : "";
  const text = generateSmartResponse(message, category, modelUsed, prefix);
  return { text, model: modelUsed };
}

/** Attempt real LLM API call using user's keys */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function tryLlmCall(message: string, category: string, strategy: string, keys: any[]): Promise<AIResponse | null> {
  const systemPrompt = CATEGORY_SYSTEM_PROMPTS[category] ?? CATEGORY_SYSTEM_PROMPTS.other;
  const skills = CATEGORY_SKILLS[category] ?? CATEGORY_SKILLS.other;
  const enrichedSystem = `${systemPrompt}\n\nAvailable skills for this category: ${skills.join(", ")}`;

  // Check env-level keys first (MoA-provided credits)
  const envAnthropicKey = process.env.ANTHROPIC_API_KEY;
  const envOpenaiKey = process.env.OPENAI_API_KEY;
  const envGeminiKey = process.env.GEMINI_API_KEY;

  // User-provided keys
  const userAnthropicKey = keys.find((k: { provider: string }) => k.provider === "anthropic")?.encrypted_key;
  const userOpenaiKey = keys.find((k: { provider: string }) => k.provider === "openai")?.encrypted_key;
  const userGeminiKey = keys.find((k: { provider: string }) => k.provider === "gemini")?.encrypted_key;
  const userGroqKey = keys.find((k: { provider: string }) => k.provider === "groq")?.encrypted_key;
  const userDeepseekKey = keys.find((k: { provider: string }) => k.provider === "deepseek")?.encrypted_key;

  // Max-performance: use the best model available
  if (strategy === "max-performance") {
    const anthropicKey = userAnthropicKey ?? envAnthropicKey;
    if (anthropicKey) {
      const result = await callAnthropic(anthropicKey, enrichedSystem, message, "claude-opus-4-6");
      if (result) return { text: result, model: "anthropic/claude-opus-4-6" };
    }
    const openaiKey = userOpenaiKey ?? envOpenaiKey;
    if (openaiKey) {
      const result = await callOpenAI(openaiKey, enrichedSystem, message, "gpt-5");
      if (result) return { text: result, model: "openai/gpt-5" };
    }
  }

  // Cost-efficient: try cheaper models first
  if (userGroqKey) {
    const result = await callGroq(userGroqKey, enrichedSystem, message);
    if (result) return { text: result, model: "groq/kimi-k2-0905" };
  }

  const geminiKey = userGeminiKey ?? envGeminiKey;
  if (geminiKey) {
    const result = await callGemini(geminiKey, enrichedSystem, message);
    if (result) return { text: result, model: "gemini/gemini-2.5-flash" };
  }

  if (userDeepseekKey) {
    const result = await callDeepSeek(userDeepseekKey, enrichedSystem, message);
    if (result) return { text: result, model: "deepseek/deepseek-chat" };
  }

  return null;
}

async function callAnthropic(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: { "Content-Type": "application/json", "x-api-key": key, "anthropic-version": "2023-06-01" },
      body: JSON.stringify({ model, max_tokens: 4096, system, messages: [{ role: "user", content: message }] }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.content?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callOpenAI(key: string, system: string, message: string, model: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model, messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callGemini(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${key}`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ contents: [{ parts: [{ text: `${system}\n\n${message}` }] }], generationConfig: { maxOutputTokens: 4096 } }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.candidates?.[0]?.content?.parts?.[0]?.text ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callGroq(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "llama-3.3-70b-versatile", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

async function callDeepSeek(key: string, system: string, message: string): Promise<string | null> {
  try {
    const res = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: `Bearer ${key}` },
      body: JSON.stringify({ model: "deepseek-chat", messages: [{ role: "system", content: system }, { role: "user", content: message }], max_tokens: 4096 }),
    });
    if (res.ok) {
      const data = await res.json();
      return data.choices?.[0]?.message?.content ?? null;
    }
  } catch { /* fall through */ }
  return null;
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
function selectModelName(strategy: string, keys: any[]): string {
  if (strategy === "max-performance") {
    if (keys.some((k: { provider: string }) => k.provider === "anthropic")) return "anthropic/claude-opus-4-6";
    if (keys.some((k: { provider: string }) => k.provider === "openai")) return "openai/gpt-5";
  }
  if (keys.some((k: { provider: string }) => k.provider === "groq")) return "groq/kimi-k2-0905";
  if (keys.some((k: { provider: string }) => k.provider === "gemini")) return "gemini/gemini-2.5-flash";
  if (keys.some((k: { provider: string }) => k.provider === "deepseek")) return "deepseek/deepseek-chat";
  return "local/slm-default";
}

function generateSmartResponse(message: string, category: string, model: string, prefix: string): string {
  const lowerMsg = message.toLowerCase();
  const catInfo = CATEGORY_SKILLS[category]?.join(", ") ?? "general";

  if (/^(ì•ˆë…•|hi|hello|í•˜ì´|ë°˜ê°€ì›Œ|í—¬ë¡œ)/.test(lowerMsg)) {
    return `${prefix}ì•ˆë…•í•˜ì„¸ìš”! MoA AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\n\ní˜„ì¬ ëª¨ë“œ: **${getCategoryLabel(category)}**\nì‚¬ìš© ëª¨ë¸: ${model}\ní™œì„± ìŠ¤í‚¬: ${catInfo}\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?`;
  }

  if (/^(ë„ì›€|help|ë­ í•  ìˆ˜|ê¸°ëŠ¥|ìŠ¤í‚¬)/.test(lowerMsg)) {
    return getCategoryHelp(category, prefix);
  }

  if (/ë‚ ì”¨|weather|ê¸°ì˜¨/.test(lowerMsg)) {
    return `${prefix}ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.\n\nğŸŒ¤ï¸ **ì˜¤ëŠ˜ì˜ ë‚ ì”¨** (ì„œìš¸ ê¸°ì¤€)\n- í˜„ì¬ ê¸°ì˜¨: 3Â°C\n- ìµœê³ /ìµœì €: 7Â°C / -1Â°C\n- ìŠµë„: 45%\n- ë¯¸ì„¸ë¨¼ì§€: ë³´í†µ\n\nì •í™•í•œ ì‹¤ì‹œê°„ ë‚ ì”¨ëŠ” ë‚ ì”¨ ìŠ¤í‚¬ì„ í†µí•´ ì œê³µë©ë‹ˆë‹¤.`;
  }

  if (/ì „ëµ|strategy|ëª¨ë¸|ê°€ì„±ë¹„|ìµœëŒ€ì„±ëŠ¥/.test(lowerMsg)) {
    return `${prefix}í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ ì „ëµ ì •ë³´ì…ë‹ˆë‹¤:\n\nì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: **${model}**\nì¹´í…Œê³ ë¦¬: **${getCategoryLabel(category)}**\n\nğŸ“Š **ê°€ì„±ë¹„ ì „ëµ** (ê¸°ë³¸)\n1. ë¬´ë£Œ SLM â†’ 2. Groq/Gemini ë¬´ë£Œ í•œë„ â†’ 3. DeepSeek/Kimi â†’ 4. Opus/GPT-5\n\nğŸ§  **ìµœëŒ€ì„±ëŠ¥ ì „ëµ**\n1. Claude Opus 4.6 / GPT-5 â†’ 2. ë³‘ë ¬ ë©€í‹° ëª¨ë¸`;
  }

  return `${prefix}ë„¤, ë§ì”€í•˜ì‹  ë‚´ìš©ì„ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.\n\n> "${message}"\n\ní˜„ì¬ **${getCategoryLabel(category)}** ëª¨ë“œì—ì„œ **${model}** ëª¨ë¸ë¡œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.\ní™œì„± ìŠ¤í‚¬: ${catInfo}\n\në” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!`;
}

function getCategoryLabel(category: string): string {
  const labels: Record<string, string> = {
    daily: "ì¼ìƒë¹„ì„œ", work: "ì—…ë¬´ë³´ì¡°", document: "ë¬¸ì„œì‘ì—…",
    coding: "ì½”ë”©ì‘ì—…", image: "ì´ë¯¸ì§€ì‘ì—…", music: "ìŒì•…ì‘ì—…", other: "ê¸°íƒ€",
  };
  return labels[category] ?? "ê¸°íƒ€";
}

function getCategoryHelp(category: string, prefix: string): string {
  const helps: Record<string, string> = {
    daily: `${prefix}**ì¼ìƒë¹„ì„œ** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸŒ¤ï¸ ë‚ ì”¨ ì¡°íšŒ Â· ğŸ“… ì¼ì • ê´€ë¦¬ Â· ğŸŒ ë²ˆì—­ Â· ğŸ” ì›¹ ê²€ìƒ‰\nğŸ“° ë‰´ìŠ¤ Â· ğŸ—ºï¸ ë§›ì§‘/ì¥ì†Œ ê²€ìƒ‰ Â· â° ì•ŒëŒ Â· ğŸ’¡ ìƒí™œ íŒ`,
    work: `${prefix}**ì—…ë¬´ë³´ì¡°** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“§ ì´ë©”ì¼ ì‘ì„± Â· ğŸ“Š ë°ì´í„° ë¶„ì„ Â· ğŸ“ íšŒì˜ë¡ ì •ë¦¬\nğŸ“ˆ ë³´ê³ ì„œ ì‘ì„± Â· ğŸ“‹ Notion/Airtable ì—°ë™ Â· ğŸ’¬ Slack ì—°ë™`,
    document: `${prefix}**ë¬¸ì„œì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ“‹ ë¬¸ì„œ ìš”ì•½ Â· ğŸ“‘ ì¢…í•©ë¬¸ì„œ ì‘ì„± Â· ğŸ“„ í˜•ì‹ ë³€í™˜ (DOCX/HWPX/XLSX/PDF)\nğŸ¯ PPTX ìƒì„± Â· âœï¸ TipTap ì—ë””í„° Â· ğŸ”„ OCR ë³€í™˜`,
    coding: `${prefix}**ì½”ë”©ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ”§ ì½”ë“œ ì‘ì„± Â· ğŸ› ë””ë²„ê¹… Â· ğŸ“– ì½”ë“œ ë¦¬ë·°\nğŸ”„ ìë™ì½”ë”© (ì—ëŸ¬ ìë™ ìˆ˜ì •) Â· ğŸ–¥ï¸ Vision ê¸°ë°˜ UI ê²€ì¦ Â· ğŸ“¦ GitHub ì—°ë™`,
    image: `${prefix}**ì´ë¯¸ì§€ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ–¼ï¸ AI ì´ë¯¸ì§€ ìƒì„± (FAL AI) Â· âœ‚ï¸ ì´ë¯¸ì§€ í¸ì§‘\nğŸ” ì´ë¯¸ì§€ ë¶„ì„ (Vision) Â· ğŸ­ ìŠ¤íƒ€ì¼ ë³€í™˜ Â· ğŸ“ ë¦¬ì‚¬ì´ì¦ˆ/í¬ë§· ë³€í™˜`,
    music: `${prefix}**ìŒì•…ì‘ì—…** ëª¨ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:\n\nğŸ¼ AI ì‘ê³¡ Â· ğŸ¤ ê°€ì‚¬ ì‘ì„± Â· ğŸ”Š TTS ìŒì„± í•©ì„±\nğŸ¹ ìŒì•… ë¶„ì„ Â· ğŸ™ï¸ íŒŸìºìŠ¤íŠ¸ ìƒì„±`,
    other: `${prefix}**MoA**ê°€ ì§€ì›í•˜ëŠ” ì£¼ìš” ê¸°ëŠ¥:\n\nğŸ” ê²€ìƒ‰ Â· ğŸ“‹ ë¬¸ì„œ Â· ğŸ¨ ì´ë¯¸ì§€ Â· ğŸ’» ì½”ë”© Â· ğŸµ ìŒì•…\nğŸ“¡ 15ê°œ ì±„ë„ ì—°ë™ Â· 100+ ì „ë¬¸ ìŠ¤í‚¬ Â· ë‹¤ì¤‘ LLM ì§€ì›`,
  };
  return helps[category] ?? helps.other;
}
