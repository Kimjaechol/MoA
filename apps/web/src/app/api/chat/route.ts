import { NextRequest, NextResponse } from "next/server";
import { getServiceSupabase } from "@/lib/supabase";

/**
 * POST /api/chat
 * Send a message and get an AI response.
 * Body: { user_id, session_id, content, channel? }
 */
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { user_id, session_id, content, channel = "web" } = body;

    if (!user_id || !session_id) {
      return NextResponse.json({ error: "user_id and session_id are required" }, { status: 400 });
    }
    if (!content || typeof content !== "string" || !content.trim()) {
      return NextResponse.json({ error: "Message content is required" }, { status: 400 });
    }

    const supabase = getServiceSupabase();

    // 1. Save user message
    const { error: saveError } = await supabase.from("moa_chat_messages").insert({
      user_id,
      session_id,
      role: "user",
      content: content.trim(),
      channel,
    });

    if (saveError) {
      return NextResponse.json({ error: "Failed to save message" }, { status: 500 });
    }

    // 2. Generate AI response
    // In production, this calls the MoA gateway agent system with the user's
    // model strategy and API keys. For now, we route through a smart response generator.
    const aiResponse = await generateResponse(content.trim(), user_id, supabase);

    // 3. Save AI response
    const { error: aiSaveError } = await supabase.from("moa_chat_messages").insert({
      user_id,
      session_id,
      role: "assistant",
      content: aiResponse.text,
      channel,
      model_used: aiResponse.model,
    });

    if (aiSaveError) {
      return NextResponse.json({ error: "Failed to save AI response" }, { status: 500 });
    }

    return NextResponse.json({
      reply: aiResponse.text,
      model: aiResponse.model,
      timestamp: new Date().toISOString(),
    });
  } catch {
    return NextResponse.json({ error: "Internal server error" }, { status: 500 });
  }
}

/**
 * GET /api/chat?user_id=xxx&session_id=yyy
 * Fetch chat history for a session.
 */
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const userId = searchParams.get("user_id");
    const sessionId = searchParams.get("session_id");
    const limit = parseInt(searchParams.get("limit") ?? "50", 10);

    if (!userId || !sessionId) {
      return NextResponse.json({ error: "user_id and session_id required" }, { status: 400 });
    }

    const supabase = getServiceSupabase();

    const { data, error } = await supabase
      .from("moa_chat_messages")
      .select("id, role, content, model_used, created_at")
      .eq("user_id", userId)
      .eq("session_id", sessionId)
      .order("created_at", { ascending: true })
      .limit(limit);

    if (error) {
      return NextResponse.json({ error: "Failed to fetch messages" }, { status: 500 });
    }

    return NextResponse.json({ messages: data ?? [] });
  } catch {
    return NextResponse.json({ error: "Internal server error" }, { status: 500 });
  }
}

/* -----------------------------------------------------------------
   AI Response Generator
   In production, this connects to the MoA gateway's agent dispatch
   system. The gateway routes through the user's model strategy
   (cost-efficient or max-performance) using their API keys.
   ----------------------------------------------------------------- */

interface AIResponse {
  text: string;
  model: string;
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
async function generateResponse(message: string, userId: string, supabase: any): Promise<AIResponse> {
  // Check if user has API keys configured
  const { data: keys } = await supabase
    .from("moa_user_api_keys")
    .select("provider, is_active")
    .eq("user_id", userId)
    .eq("is_active", true);

  const { data: settings } = await supabase
    .from("moa_user_settings")
    .select("model_strategy")
    .eq("user_id", userId)
    .single();

  const strategy = settings?.model_strategy ?? "cost-efficient";
  const hasGroqKey = keys?.some((k: { provider: string }) => k.provider === "groq");
  const hasGeminiKey = keys?.some((k: { provider: string }) => k.provider === "gemini");
  const hasOpenaiKey = keys?.some((k: { provider: string }) => k.provider === "openai");
  const hasAnthropicKey = keys?.some((k: { provider: string }) => k.provider === "anthropic");

  // Determine which model tier to use based on strategy
  let modelUsed = "local/slm-default";
  let responsePrefix = "";

  if (strategy === "max-performance") {
    if (hasAnthropicKey) {
      modelUsed = "anthropic/claude-opus-4-5";
    } else if (hasOpenaiKey) {
      modelUsed = "openai/gpt-5.2";
    } else {
      modelUsed = "local/slm-default";
      responsePrefix = "[ë¬´ë£Œ SLM] ";
    }
  } else {
    // Cost-efficient: try tiers in order
    if (hasGroqKey) {
      modelUsed = "groq/kimi-k2-0905";
    } else if (hasGeminiKey) {
      modelUsed = "gemini/gemini-2.5-flash";
    } else {
      modelUsed = "local/slm-default";
      responsePrefix = "[ë¬´ë£Œ SLM] ";
    }
  }

  // In production, the actual API call goes to the MoA gateway which
  // dispatches to the selected model. For the web demo, we generate
  // a smart contextual response.
  const text = generateSmartResponse(message, modelUsed, responsePrefix);

  return { text, model: modelUsed };
}

function generateSmartResponse(message: string, model: string, prefix: string): string {
  const lowerMsg = message.toLowerCase();

  // Greeting patterns
  if (/^(ì•ˆë…•|hi|hello|í•˜ì´|ë°˜ê°€ì›Œ|í—¬ë¡œ)/.test(lowerMsg)) {
    return `${prefix}ì•ˆë…•í•˜ì„¸ìš”! MoA AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n\ní˜„ì¬ ì‚¬ìš© ëª¨ë¸: ${model}\n\nê°€ëŠ¥í•œ ì‘ì—…:\n- ì§ˆë¬¸ì— ë‹µë³€\n- ì›¹ ê²€ìƒ‰\n- ë¬¸ì„œ ìš”ì•½\n- ì´ë¯¸ì§€ ìƒì„±\n- ì½”ë“œ ì‘ì„±\n- ë²ˆì—­\n\në¬´ì—‡ì´ë“  í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!`;
  }

  // Help patterns
  if (/^(ë„ì›€|help|ë­ í•  ìˆ˜|ê¸°ëŠ¥|ìŠ¤í‚¬)/.test(lowerMsg)) {
    return `${prefix}MoAê°€ ì§€ì›í•˜ëŠ” ì£¼ìš” ê¸°ëŠ¥ì…ë‹ˆë‹¤:\n\nğŸ” **ê²€ìƒ‰ & ì •ë³´**\nBrave Search, Perplexity, Google Search, ë‰´ìŠ¤, ë‚ ì”¨, ë¯¸ì„¸ë¨¼ì§€\n\nğŸ“‹ **ìƒì‚°ì„± & ì—…ë¬´**\nNotion, Airtable, Slack, GitHub, ìº˜ë¦°ë”, ìš”ì•½\n\nğŸ¨ **ë¯¸ë””ì–´ ìƒì„±**\nFAL AI ì´ë¯¸ì§€, Gamma í”„ë ˆì  í…Œì´ì…˜, íŒŸìºìŠ¤íŠ¸, TTS\n\nğŸ¤– **AI & ë¨¸ì‹ ëŸ¬ë‹**\nGemini, HuggingFace, Replicate, ChromaDB\n\nğŸ›¡ï¸ **ë³´ì•ˆ & ì‹œìŠ¤í…œ**\në³´ì•ˆ ì ê²€, í™ˆ ì–´ì‹œìŠ¤í„´íŠ¸, ëª¨ë‹ˆí„°ë§\n\ní˜„ì¬ **100ê°œ ì´ìƒì˜ ì „ë¬¸ ìŠ¤í‚¬**ì´ íƒ‘ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤!`;
  }

  // Weather patterns
  if (/ë‚ ì”¨|weather|ê¸°ì˜¨|ë¹„ ì˜¬/.test(lowerMsg)) {
    return `${prefix}ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.\n\nğŸŒ¤ï¸ **ì˜¤ëŠ˜ì˜ ë‚ ì”¨** (ì„œìš¸ ê¸°ì¤€)\n- í˜„ì¬ ê¸°ì˜¨: 3Â°C\n- ìµœê³ /ìµœì €: 7Â°C / -1Â°C\n- ìŠµë„: 45%\n- ë¯¸ì„¸ë¨¼ì§€: ë³´í†µ\n\nì •í™•í•œ ë‚ ì”¨ ì •ë³´ëŠ” ë‚ ì”¨ ìŠ¤í‚¬ì„ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.\nì›í•˜ì‹œëŠ” ì§€ì—­ì„ ì•Œë ¤ì£¼ì‹œë©´ í•´ë‹¹ ì§€ì—­ì˜ ë‚ ì”¨ë¥¼ ì¡°íšŒí•´ë“œë¦½ë‹ˆë‹¤.`;
  }

  // Strategy info
  if (/ì „ëµ|strategy|ëª¨ë¸|ê°€ì„±ë¹„|ìµœëŒ€ì„±ëŠ¥/.test(lowerMsg)) {
    return `${prefix}í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ ì „ëµ ì •ë³´ì…ë‹ˆë‹¤:\n\nì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: **${model}**\n\nğŸ“Š **ê°€ì„±ë¹„ ì „ëµ** (ê¸°ë³¸)\n1. ë¬´ë£Œ ë‚´ì¥ SLM â†’ 2. ìœ ë£Œ LLM ë¬´ë£Œ í•œë„ â†’ 3. Kimi K2-0905 Groq ë“± ê°€ì„±ë¹„ â†’ 4. ìµœê³ ê¸‰ LLM\n\nğŸ§  **ìµœëŒ€ì„±ëŠ¥ ì „ëµ**\n1. ìµœê³  ì„±ëŠ¥ ë‹¨ì¼ ëª¨ë¸ â†’ 2. ë³‘ë ¬ ë©€í‹° ëª¨ë¸\n\nì „ëµì€ ë§ˆì´í˜ì´ì§€ì—ì„œ ì–¸ì œë“  ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`;
  }

  // Channel info
  if (/ì±„ë„|channel|ì¹´ì¹´ì˜¤|í…”ë ˆê·¸ë¨|telegram|whatsapp|discord/.test(lowerMsg)) {
    return `${prefix}MoAëŠ” 15ê°œ ì±„ë„ì„ ì§€ì›í•©ë‹ˆë‹¤:\n\nğŸ’¬ **ì£¼ìš” ì±„ë„**\n- ì¹´ì¹´ì˜¤í†¡ Â· í…”ë ˆê·¸ë¨ Â· Discord Â· WhatsApp\n- Slack Â· Signal Â· iMessage Â· LINE\n\nğŸŒ **ì¶”ê°€ ì±„ë„**\n- MS Teams Â· Matrix Â· Google Chat Â· Mattermost\n- Twitch Â· Nostr Â· Zalo\n\nê° ì±„ë„ì˜ "ëŒ€í™” ì‹œì‘í•˜ê¸°" ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ë°”ë¡œ ì—°ê²°ë©ë‹ˆë‹¤!\nì±„ë„ í—ˆë¸Œ(/channels)ì—ì„œ ëª¨ë“  ì±„ë„ì„ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`;
  }

  // Default response
  return `${prefix}ë„¤, ë§ì”€í•˜ì‹  ë‚´ìš©ì„ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.\n\n> "${message}"\n\ní˜„ì¬ **${model}** ëª¨ë¸ë¡œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.\n\nMoAëŠ” ì¹´ì¹´ì˜¤í†¡, í…”ë ˆê·¸ë¨, Discord ë“± 15ê°œ ì±„ë„ì—ì„œ ë™ì¼í•œ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ì±„ë„ì—ì„œë“  ë™ì¼í•œ AI ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n\në” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!`;
}
