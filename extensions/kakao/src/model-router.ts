/**
 * Model Router - Multi-Provider LLM Routing
 *
 * Routes requests to appropriate LLM providers with:
 * - User API key priority
 * - Platform API fallback
 * - Free tier automatic switching
 * - Rate limit handling
 * - Complexity-based model selection (NEW)
 * - Privacy-aware local SLM routing (NEW)
 */

import {
  type LLMProvider,
  type ResolvedModel,
  resolveModel,
  getUserSettings,
  PROVIDERS,
  FREE_MODELS,
} from "./user-settings.js";
import { getCredits } from "./billing.js";
import {
  classifyComplexity,
  buildPremiumModelNotification,
  type ComplexityResult,
  type SuggestedModelTier,
} from "./complexity-classifier.js";
import {
  classifyPrivacy,
  canSendToExternalAPI,
  maskSensitiveData,
  type PrivacyResult,
} from "./privacy-classifier.js";
import {
  processThroughSLM,
  getMoAAgentStatus,
  type SLMRequest,
} from "./slm/index.js";

// ============================================
// Types
// ============================================

export interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

export interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
}

export interface ChatResponse {
  content: string;
  model: string;
  provider: LLMProvider;
  usage: {
    inputTokens: number;
    outputTokens: number;
  };
  isFallback: boolean;
  isFree: boolean;
}

export interface RouterResult {
  success: boolean;
  response?: ChatResponse;
  error?: string;
  fallbackUsed?: boolean;
  fallbackProvider?: LLMProvider;
}

// ============================================
// Smart Routing Types (NEW)
// ============================================

export interface SmartRoutingAnalysis {
  complexity: ComplexityResult;
  privacy: PrivacyResult;
  suggestedTier: SuggestedModelTier;
  requiresUserConfirmation: boolean;
  requiresLocalProcessing: boolean;
}

export interface SmartRouterResult extends RouterResult {
  analysis?: SmartRoutingAnalysis;
  notificationMessage?: string;
  usedPremiumModel?: boolean;
  localProcessingRequired?: boolean;
}

export type UserConfirmationAction =
  | "use_premium" // ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš© (í¬ë ˆë”§ ì°¨ê°)
  | "use_free" // ë¬´ë£Œ ëª¨ë¸ë¡œ ì‹œë„
  | "register_api_key" // API í‚¤ ë“±ë¡í•˜ëŸ¬ ê°€ê¸°
  | "cancel"; // ì·¨ì†Œ

export interface PendingPremiumRequest {
  kakaoUserId: string;
  originalMessage: string;
  analysis: SmartRoutingAnalysis;
  createdAt: Date;
  expiresAt: Date;
}

// ============================================
// Provider API Callers
// ============================================

async function callAnthropic(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  const response = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "x-api-key": apiKey,
      "anthropic-version": "2023-06-01",
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      messages: messages.filter(m => m.role !== "system").map(m => ({
        role: m.role,
        content: m.content,
      })),
      system: messages.find(m => m.role === "system")?.content,
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `Anthropic API error: ${response.status}`);
  }

  const data = await response.json() as {
    content: Array<{ type: string; text?: string }>;
    usage: { input_tokens: number; output_tokens: number };
  };

  const textContent = data.content.find(c => c.type === "text");

  return {
    content: textContent?.text ?? "",
    model,
    provider: "anthropic",
    usage: {
      inputTokens: data.usage.input_tokens,
      outputTokens: data.usage.output_tokens,
    },
    isFallback: false,
    isFree: false,
  };
}

async function callOpenAI(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `OpenAI API error: ${response.status}`);
  }

  const data = await response.json() as {
    choices: Array<{ message: { content: string } }>;
    usage: { prompt_tokens: number; completion_tokens: number };
  };

  return {
    content: data.choices[0]?.message?.content ?? "",
    model,
    provider: "openai",
    usage: {
      inputTokens: data.usage.prompt_tokens,
      outputTokens: data.usage.completion_tokens,
    },
    isFallback: false,
    isFree: false,
  };
}

async function callGoogle(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  // Convert messages to Gemini format
  const contents = [];
  let systemInstruction: string | undefined;

  for (const msg of messages) {
    if (msg.role === "system") {
      systemInstruction = msg.content;
    } else {
      contents.push({
        role: msg.role === "assistant" ? "model" : "user",
        parts: [{ text: msg.content }],
      });
    }
  }

  const requestBody: Record<string, unknown> = {
    contents,
    generationConfig: {
      maxOutputTokens: maxTokens,
    },
  };

  if (systemInstruction) {
    requestBody.systemInstruction = { parts: [{ text: systemInstruction }] };
  }

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(requestBody),
    },
  );

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `Google API error: ${response.status}`);
  }

  const data = await response.json() as {
    candidates: Array<{ content: { parts: Array<{ text: string }> } }>;
    usageMetadata?: { promptTokenCount: number; candidatesTokenCount: number };
  };

  const textParts = data.candidates?.[0]?.content?.parts?.filter(p => p.text) ?? [];
  const content = textParts.map(p => p.text).join("");

  return {
    content,
    model,
    provider: "google",
    usage: {
      inputTokens: data.usageMetadata?.promptTokenCount ?? 0,
      outputTokens: data.usageMetadata?.candidatesTokenCount ?? 0,
    },
    isFallback: false,
    isFree: true, // Gemini has generous free tier
  };
}

async function callGroq(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `Groq API error: ${response.status}`);
  }

  const data = await response.json() as {
    choices: Array<{ message: { content: string } }>;
    usage: { prompt_tokens: number; completion_tokens: number };
  };

  return {
    content: data.choices[0]?.message?.content ?? "",
    model,
    provider: "groq",
    usage: {
      inputTokens: data.usage.prompt_tokens,
      outputTokens: data.usage.completion_tokens,
    },
    isFallback: false,
    isFree: true, // Groq is free
  };
}

async function callTogether(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  const response = await fetch("https://api.together.xyz/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `Together API error: ${response.status}`);
  }

  const data = await response.json() as {
    choices: Array<{ message: { content: string } }>;
    usage: { prompt_tokens: number; completion_tokens: number };
  };

  return {
    content: data.choices[0]?.message?.content ?? "",
    model,
    provider: "together",
    usage: {
      inputTokens: data.usage.prompt_tokens,
      outputTokens: data.usage.completion_tokens,
    },
    isFallback: false,
    isFree: false,
  };
}

async function callOpenRouter(
  apiKey: string,
  model: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
      "HTTP-Referer": "https://kakaomolt.com",
      "X-Title": "KakaoMolt",
    },
    body: JSON.stringify({
      model,
      max_tokens: maxTokens,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
    }),
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: { message: response.statusText } }));
    throw new Error(error.error?.message ?? `OpenRouter API error: ${response.status}`);
  }

  const data = await response.json() as {
    choices: Array<{ message: { content: string } }>;
    usage: { prompt_tokens: number; completion_tokens: number };
  };

  const isFreeModel = model.includes(":free");

  return {
    content: data.choices[0]?.message?.content ?? "",
    model,
    provider: "openrouter",
    usage: {
      inputTokens: data.usage.prompt_tokens,
      outputTokens: data.usage.completion_tokens,
    },
    isFallback: false,
    isFree: isFreeModel,
  };
}

// ============================================
// Main Router
// ============================================

/**
 * Call LLM provider
 */
async function callProvider(
  resolved: ResolvedModel,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse> {
  switch (resolved.provider) {
    case "anthropic":
      return callAnthropic(resolved.apiKey, resolved.model, messages, maxTokens);
    case "openai":
      return callOpenAI(resolved.apiKey, resolved.model, messages, maxTokens);
    case "google":
      return callGoogle(resolved.apiKey, resolved.model, messages, maxTokens);
    case "groq":
      return callGroq(resolved.apiKey, resolved.model, messages, maxTokens);
    case "together":
      return callTogether(resolved.apiKey, resolved.model, messages, maxTokens);
    case "openrouter":
      return callOpenRouter(resolved.apiKey, resolved.model, messages, maxTokens);
    default:
      throw new Error(`Unsupported provider: ${resolved.provider}`);
  }
}

/**
 * Route chat request to appropriate provider
 */
export async function routeChat(
  kakaoUserId: string,
  request: ChatRequest,
): Promise<RouterResult> {
  const maxTokens = request.maxTokens ?? 4096;

  // Get user's credit balance
  const credits = await getCredits(kakaoUserId);
  const hasCredits = credits > 0;

  // Resolve which model to use
  const resolved = await resolveModel(kakaoUserId, hasCredits);

  if ("error" in resolved) {
    return {
      success: false,
      error: resolved.error,
    };
  }

  try {
    const response = await callProvider(resolved, request.messages, maxTokens);

    // Update response with fallback info
    response.isFallback = resolved.isFallback;
    response.isFree = resolved.isFree;

    return {
      success: true,
      response,
      fallbackUsed: resolved.isFallback,
      fallbackProvider: resolved.isFallback ? resolved.provider : undefined,
    };
  } catch (err) {
    // If primary provider fails, try fallback
    if (!resolved.isFallback) {
      const fallbackResult = await tryFallbackProviders(kakaoUserId, request.messages, maxTokens);
      if (fallbackResult) {
        return {
          success: true,
          response: fallbackResult,
          fallbackUsed: true,
          fallbackProvider: fallbackResult.provider,
        };
      }
    }

    return {
      success: false,
      error: err instanceof Error ? err.message : "LLM ìš”ì²­ ì‹¤íŒ¨",
    };
  }
}

/**
 * Try fallback providers when primary fails
 *
 * í´ë°± ìˆœì„œ:
 * 1. ë¬´ë£Œ ëª¨ë¸ (Gemini Flash â†’ Groq â†’ OpenRouter ë¬´ë£Œ)
 * 2. ìœ ë£Œ ëª¨ë¸ ê°€ì„±ë¹„ìˆœ (Gemini Pro â†’ GPT-4o Mini â†’ Claude Haiku â†’ ...)
 */
async function tryFallbackProviders(
  kakaoUserId: string,
  messages: ChatMessage[],
  maxTokens: number,
): Promise<ChatResponse | null> {
  const settings = await getUserSettings(kakaoUserId);
  const credits = await getCredits(kakaoUserId);
  const hasCredits = credits > 0;

  // 1ë‹¨ê³„: ë¬´ë£Œ ëª¨ë¸ ë¨¼ì € ì‹œë„
  const freeFallbacks: Array<{ provider: LLMProvider; model: string }> = [
    { provider: "google", model: "gemini-2.0-flash" },
    { provider: "groq", model: "llama-3.3-70b-versatile" },
    { provider: "openrouter", model: "google/gemini-2.0-flash-exp:free" },
  ];

  for (const fallback of freeFallbacks) {
    const apiKey = settings.apiKeys[fallback.provider] ?? getPlatformKey(fallback.provider);
    if (!apiKey) continue;

    try {
      const response = await callProvider(
        { provider: fallback.provider, model: fallback.model, apiKey, isFallback: true, isFree: true },
        messages, maxTokens,
      );
      response.isFallback = true;
      response.isFree = true;
      return response;
    } catch {
      continue;
    }
  }

  // 2ë‹¨ê³„: ìœ ë£Œ ëª¨ë¸ ê°€ì„±ë¹„ìˆœ (ì‚¬ìš©ì í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ í”Œë«í¼ í‚¤ + í¬ë ˆë”§)
  const paidFallbacks: Array<{ provider: LLMProvider; model: string }> = [
    { provider: "google", model: "gemini-1.5-pro" },
    { provider: "openai", model: "gpt-4o-mini" },
    { provider: "anthropic", model: "claude-3-5-haiku-latest" },
    { provider: "together", model: "meta-llama/Llama-3.3-70B-Instruct-Turbo" },
    { provider: "openai", model: "gpt-4o" },
    { provider: "anthropic", model: "claude-sonnet-4-20250514" },
  ];

  for (const fallback of paidFallbacks) {
    // ì‚¬ìš©ì í‚¤ê°€ ìˆìœ¼ë©´ ë¬´ë£Œ (isFree=true)
    const userKey = settings.apiKeys[fallback.provider];
    if (userKey) {
      try {
        const response = await callProvider(
          { provider: fallback.provider, model: fallback.model, apiKey: userKey, isFallback: true, isFree: true },
          messages, maxTokens,
        );
        response.isFallback = true;
        response.isFree = true;
        return response;
      } catch {
        continue;
      }
    }

    // ì‚¬ìš©ì í‚¤ ì—†ìœ¼ë©´ í”Œë«í¼ API ì‚¬ìš© (í¬ë ˆë”§ ì°¨ê°, 2ë°°)
    if (hasCredits) {
      const platformKey = getPlatformKey(fallback.provider);
      if (platformKey) {
        try {
          const response = await callProvider(
            { provider: fallback.provider, model: fallback.model, apiKey: platformKey, isFallback: true, isFree: false },
            messages, maxTokens,
          );
          response.isFallback = true;
          response.isFree = false;
          return response;
        } catch {
          continue;
        }
      }
    }
  }

  return null;
}

/**
 * Get platform API key
 */
function getPlatformKey(provider: LLMProvider): string | undefined {
  switch (provider) {
    case "anthropic":
      return process.env.ANTHROPIC_API_KEY;
    case "openai":
      return process.env.OPENAI_API_KEY;
    case "google":
      return process.env.GOOGLE_API_KEY ?? process.env.GEMINI_API_KEY;
    case "groq":
      return process.env.GROQ_API_KEY;
    case "together":
      return process.env.TOGETHER_API_KEY;
    case "openrouter":
      return process.env.OPENROUTER_API_KEY;
    default:
      return undefined;
  }
}

// ============================================
// Utility Functions
// ============================================

/**
 * Get friendly provider name
 */
export function getProviderDisplayName(provider: LLMProvider): string {
  return PROVIDERS[provider]?.displayName ?? provider;
}

/**
 * Get friendly model name
 */
export function getModelDisplayName(provider: LLMProvider, modelId: string): string {
  const model = PROVIDERS[provider]?.models.find(m => m.id === modelId);
  return model?.name ?? modelId;
}

/**
 * Format response with provider info
 */
export function formatResponseWithInfo(result: RouterResult): string {
  if (!result.success || !result.response) {
    return result.error ?? "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
  }

  let text = result.response.content;

  // Add fallback notice if used
  if (result.fallbackUsed && result.fallbackProvider) {
    const providerName = getProviderDisplayName(result.fallbackProvider);
    text += `\n\nğŸ’¡ _${providerName} ë¬´ë£Œ ëª¨ë¸ë¡œ ìë™ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤._`;
  }

  return text;
}

/**
 * Get warning message when credits are low
 */
export function getLowCreditWarning(credits: number, hasApiKey: boolean): string | null {
  if (hasApiKey) return null;

  if (credits <= 0) {
    return `âš ï¸ í¬ë ˆë”§ì´ ëª¨ë‘ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ†“ ë¬´ë£Œë¡œ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´:
1. "APIí‚¤ ë“±ë¡"ì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”
2. Google Gemini API í‚¤ ë“±ë¡ (ë¬´ë£Œ!)

ğŸ’³ ë˜ëŠ” "ì¶©ì „"ìœ¼ë¡œ í¬ë ˆë”§ì„ ì¶©ì „í•˜ì„¸ìš”.`;
  }

  if (credits < 100) {
    return `âš ï¸ í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (${credits} ë‚¨ìŒ)

ğŸ’¡ ë¬´ë£Œ API í‚¤ë¥¼ ë“±ë¡í•˜ë©´ ë¬´ë£Œë¡œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!
"APIí‚¤ ë“±ë¡"ì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”.`;
  }

  return null;
}

/**
 * Get token count estimate (rough approximation)
 */
export function estimateTokens(text: string): number {
  // Rough estimate: ~4 characters per token for Korean
  return Math.ceil(text.length / 4);
}

// ============================================
// Smart Routing (NEW)
// ============================================

// ëŒ€ê¸° ì¤‘ì¸ ê³ ê¸‰ ëª¨ë¸ ìš”ì²­ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬)
const pendingPremiumRequests = new Map<string, PendingPremiumRequest>();

/**
 * ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…: ë³µì¡ë„ + í”„ë¼ì´ë²„ì‹œ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
 *
 * 1. ë©”ì‹œì§€ ë³µì¡ë„ ë¶„ì„
 * 2. í”„ë¼ì´ë²„ì‹œ ë¯¼ê°ë„ ë¶„ì„
 * 3. ì ì ˆí•œ ëª¨ë¸ í‹°ì–´ ê²°ì •
 * 4. ì‚¬ìš©ì í™•ì¸ í•„ìš”ì‹œ ì•Œë¦¼ ìƒì„±
 */
export async function smartRouteChat(
  kakaoUserId: string,
  userMessage: string,
  request: ChatRequest,
): Promise<SmartRouterResult> {
  // 1. ë³µì¡ë„ ë¶„ì„
  const complexity = classifyComplexity(userMessage);

  // 2. í”„ë¼ì´ë²„ì‹œ ë¶„ì„
  const privacy = classifyPrivacy(userMessage);

  // 3. ë¶„ì„ ê²°ê³¼ ì¢…í•©
  const analysis: SmartRoutingAnalysis = {
    complexity,
    privacy,
    suggestedTier: privacy.shouldUseLocalSLM ? "local" : complexity.suggestedTier,
    requiresUserConfirmation: complexity.requiresUserConfirmation && !privacy.shouldUseLocalSLM,
    requiresLocalProcessing: privacy.shouldUseLocalSLM,
  };

  // 4. ë¡œì»¬ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° (ë¯¼ê° ì •ë³´)
  if (analysis.requiresLocalProcessing) {
    // Try to process through local SLM
    const agentStatus = getMoAAgentStatus();

    if (agentStatus.slmReady) {
      // Convert request to SLM format
      const slmRequest: SLMRequest = {
        messages: request.messages.map(m => ({
          role: m.role,
          content: m.content,
        })),
        maxTokens: request.maxTokens,
        temperature: request.temperature,
      };

      const slmResult = await processThroughSLM(userMessage, slmRequest, {
        forceLocal: true, // Force local for privacy
      });

      if (slmResult.success && slmResult.response) {
        return {
          success: true,
          response: {
            content: slmResult.response.content,
            model: slmResult.response.model,
            provider: "local" as LLMProvider,
            usage: {
              inputTokens: slmResult.response.usage.promptTokens,
              outputTokens: slmResult.response.usage.completionTokens,
            },
            isFallback: false,
            isFree: true, // Local processing is free
          },
          analysis,
          localProcessingRequired: true,
          notificationMessage: `ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ ë¡œì»¬ AIë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.\n${privacy.warningMessage || ""}`,
        };
      }
    }

    // Local SLM not available - warn user but allow cloud fallback with masking
    return {
      success: false,
      localProcessingRequired: true,
      analysis,
      notificationMessage: `âš ï¸ ë¯¼ê°í•œ ì •ë³´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n${privacy.warningMessage}\n\në¡œì»¬ AIê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"MoA ì„¤ì¹˜"ë¼ê³  ì…ë ¥í•˜ì—¬ ë¡œì»¬ AIë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.`,
      error: "LOCAL_PROCESSING_REQUIRED",
    };
  }

  // 5. ì‚¬ìš©ì ì„¤ì • ë° í¬ë ˆë”§ í™•ì¸
  const settings = await getUserSettings(kakaoUserId);
  const credits = await getCredits(kakaoUserId);

  // ê³ ê¸‰ ëª¨ë¸ìš© API í‚¤ í™•ì¸
  const hasPremiumApiKey =
    !!settings.apiKeys.anthropic ||
    !!settings.apiKeys.openai ||
    !!settings.apiKeys.google;

  // 6. ê³ ê¸‰ ëª¨ë¸ì´ í•„ìš”í•˜ê³  API í‚¤ê°€ ì—†ëŠ” ê²½ìš° â†’ ì‚¬ìš©ì í™•ì¸ í•„ìš”
  if (analysis.requiresUserConfirmation && !hasPremiumApiKey) {
    const notification = buildPremiumModelNotification(complexity, false, credits);

    if (notification.required) {
      // ëŒ€ê¸° ìš”ì²­ ì €ì¥
      const pending: PendingPremiumRequest = {
        kakaoUserId,
        originalMessage: userMessage,
        analysis,
        createdAt: new Date(),
        expiresAt: new Date(Date.now() + 5 * 60 * 1000), // 5ë¶„ í›„ ë§Œë£Œ
      };
      pendingPremiumRequests.set(kakaoUserId, pending);

      return {
        success: false,
        analysis,
        notificationMessage: notification.message,
        error: "PREMIUM_CONFIRMATION_REQUIRED",
      };
    }
  }

  // 7. ê³ ê¸‰ ëª¨ë¸ì´ í•„ìš”í•˜ê³  API í‚¤ê°€ ìˆëŠ” ê²½ìš° â†’ ìë™ ì‚¬ìš©
  if (analysis.suggestedTier === "premium" && hasPremiumApiKey) {
    const notification = buildPremiumModelNotification(complexity, true, credits);

    const result = await routeChatWithTier(kakaoUserId, request, "premium");

    return {
      ...result,
      analysis,
      notificationMessage: notification.message,
      usedPremiumModel: true,
    };
  }

  // 8. ì¼ë°˜ ë¼ìš°íŒ… (ë³µì¡ë„ ê¸°ë°˜)
  const result = await routeChatWithTier(kakaoUserId, request, analysis.suggestedTier);

  return {
    ...result,
    analysis,
  };
}

/**
 * íŠ¹ì • í‹°ì–´ë¡œ ë¼ìš°íŒ…
 */
async function routeChatWithTier(
  kakaoUserId: string,
  request: ChatRequest,
  tier: SuggestedModelTier,
): Promise<RouterResult> {
  const maxTokens = request.maxTokens ?? 4096;
  const settings = await getUserSettings(kakaoUserId);
  const credits = await getCredits(kakaoUserId);

  // í‹°ì–´ë³„ ëª¨ë¸ ëª©ë¡
  const tierModels: Record<SuggestedModelTier, Array<{ provider: LLMProvider; model: string }>> = {
    free: [
      { provider: "google", model: "gemini-2.0-flash" },
      { provider: "groq", model: "llama-3.3-70b-versatile" },
      { provider: "openrouter", model: "google/gemini-2.0-flash-exp:free" },
    ],
    cheap: [
      { provider: "anthropic", model: "claude-3-5-haiku-latest" },
      { provider: "openai", model: "gpt-4o-mini" },
      { provider: "google", model: "gemini-1.5-pro" },
    ],
    premium: [
      { provider: "anthropic", model: "claude-opus-4-5-20251101" },
      { provider: "openai", model: "gpt-5.2" },
      { provider: "google", model: "gemini-3-pro-preview" },
    ],
    local: [], // ë¡œì»¬ì€ ë³„ë„ ì²˜ë¦¬
  };

  const models = tierModels[tier] || tierModels.free;

  // ê° ëª¨ë¸ ì‹œë„
  for (const { provider, model } of models) {
    // ì‚¬ìš©ì í‚¤ ìš°ì„ 
    const userKey = settings.apiKeys[provider];
    if (userKey) {
      try {
        const response = await callProvider(
          { provider, model, apiKey: userKey, isFallback: false, isFree: true },
          request.messages,
          maxTokens,
        );
        return { success: true, response };
      } catch {
        continue;
      }
    }

    // í”Œë«í¼ í‚¤ (í¬ë ˆë”§ í•„ìš”)
    if (credits > 0 || tier === "free") {
      const platformKey = getPlatformKey(provider);
      if (platformKey) {
        try {
          const response = await callProvider(
            { provider, model, apiKey: platformKey, isFallback: false, isFree: tier === "free" },
            request.messages,
            maxTokens,
          );
          return { success: true, response };
        } catch {
          continue;
        }
      }
    }
  }

  // ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í´ë°±
  return routeChat(kakaoUserId, request);
}

/**
 * ì‚¬ìš©ì í™•ì¸ í›„ ê³ ê¸‰ ëª¨ë¸ ìš”ì²­ ì²˜ë¦¬
 */
export async function handlePremiumConfirmation(
  kakaoUserId: string,
  action: UserConfirmationAction,
  request?: ChatRequest,
): Promise<SmartRouterResult> {
  const pending = pendingPremiumRequests.get(kakaoUserId);

  if (!pending) {
    return {
      success: false,
      error: "ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.",
    };
  }

  // ë§Œë£Œ í™•ì¸
  if (new Date() > pending.expiresAt) {
    pendingPremiumRequests.delete(kakaoUserId);
    return {
      success: false,
      error: "ìš”ì²­ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
    };
  }

  // ëŒ€ê¸° ìš”ì²­ ì‚­ì œ
  pendingPremiumRequests.delete(kakaoUserId);

  switch (action) {
    case "use_premium": {
      // í¬ë ˆë”§ìœ¼ë¡œ ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©
      const credits = await getCredits(kakaoUserId);
      if (credits < 100) {
        return {
          success: false,
          error: "í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶©ì „ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
          notificationMessage: `ğŸ’° í˜„ì¬ ì”ì•¡: ${credits} í¬ë ˆë”§\n\n"ì¶©ì „"ì´ë¼ê³  ì…ë ¥í•˜ì—¬ í¬ë ˆë”§ì„ ì¶©ì „í•˜ì„¸ìš”.`,
        };
      }

      // ì›ë³¸ ë©”ì‹œì§€ë¡œ ìš”ì²­ ìƒì„±
      const chatRequest: ChatRequest = request ?? {
        messages: [{ role: "user", content: pending.originalMessage }],
      };

      return routeChatWithTier(kakaoUserId, chatRequest, "premium").then((result) => ({
        ...result,
        analysis: pending.analysis,
        usedPremiumModel: true,
        notificationMessage: `ğŸ§  ê³ ê¸‰ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (í¬ë ˆë”§ ì°¨ê°)`,
      }));
    }

    case "use_free": {
      // ë¬´ë£Œ ëª¨ë¸ë¡œ ì‹œë„
      const chatRequest: ChatRequest = request ?? {
        messages: [{ role: "user", content: pending.originalMessage }],
      };

      return routeChatWithTier(kakaoUserId, chatRequest, "free").then((result) => ({
        ...result,
        analysis: pending.analysis,
        notificationMessage: `ğŸ†“ ë¬´ë£Œ ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (í’ˆì§ˆì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)`,
      }));
    }

    case "register_api_key":
      return {
        success: false,
        analysis: pending.analysis,
        notificationMessage: `ğŸ”‘ API í‚¤ ë“±ë¡ ì•ˆë‚´

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë“±ë¡í•˜ì‹œë©´ ë¬´ë£Œë¡œ ê³ ê¸‰ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1ï¸âƒ£ **Anthropic Claude**
   â†’ https://console.anthropic.com
   â†’ "APIí‚¤ ë“±ë¡ anthropic sk-ant-xxx"

2ï¸âƒ£ **OpenAI GPT**
   â†’ https://platform.openai.com
   â†’ "APIí‚¤ ë“±ë¡ openai sk-xxx"

3ï¸âƒ£ **Google Gemini** (ë¬´ë£Œ!)
   â†’ https://aistudio.google.com
   â†’ "APIí‚¤ ë“±ë¡ google AIza..."`,
        error: "API_KEY_REGISTRATION",
      };

    case "cancel":
    default:
      return {
        success: false,
        analysis: pending.analysis,
        notificationMessage: "ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        error: "CANCELLED",
      };
  }
}

/**
 * ì‚¬ìš©ì ì‘ë‹µì´ ê³ ê¸‰ ëª¨ë¸ í™•ì¸ì¸ì§€ í™•ì¸
 */
export function isPremiumConfirmationResponse(message: string): UserConfirmationAction | null {
  const normalized = message.trim().toLowerCase();

  // ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©
  if (/^(ê³ ê¸‰\s*ëª¨ë¸|í”„ë¦¬ë¯¸ì—„|premium|ê³ ê¸‰|use\s*premium)/i.test(normalized)) {
    return "use_premium";
  }

  // ë¬´ë£Œë¡œ ì‹œë„
  if (/^(ë¬´ë£Œ|ë¬´ë£Œë¡œ|free|try\s*free|ë¬´ë£Œ\s*ì‹œë„)/i.test(normalized)) {
    return "use_free";
  }

  // API í‚¤ ë“±ë¡
  if (/^(api\s*í‚¤|apikey|í‚¤\s*ë“±ë¡|register)/i.test(normalized)) {
    return "register_api_key";
  }

  // ì·¨ì†Œ
  if (/^(ì·¨ì†Œ|cancel|ì•„ë‹ˆ|no)/i.test(normalized)) {
    return "cancel";
  }

  return null;
}

/**
 * ëŒ€ê¸° ì¤‘ì¸ ê³ ê¸‰ ëª¨ë¸ ìš”ì²­ í™•ì¸
 */
export function hasPendingPremiumRequest(kakaoUserId: string): boolean {
  const pending = pendingPremiumRequests.get(kakaoUserId);
  if (!pending) return false;

  // ë§Œë£Œ í™•ì¸
  if (new Date() > pending.expiresAt) {
    pendingPremiumRequests.delete(kakaoUserId);
    return false;
  }

  return true;
}

/**
 * ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… ë¶„ì„ë§Œ ìˆ˜í–‰ (ë¼ìš°íŒ… ì—†ì´)
 */
export function analyzeMessage(message: string): SmartRoutingAnalysis {
  const complexity = classifyComplexity(message);
  const privacy = classifyPrivacy(message);

  return {
    complexity,
    privacy,
    suggestedTier: privacy.shouldUseLocalSLM ? "local" : complexity.suggestedTier,
    requiresUserConfirmation: complexity.requiresUserConfirmation && !privacy.shouldUseLocalSLM,
    requiresLocalProcessing: privacy.shouldUseLocalSLM,
  };
}

/**
 * ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
 */
export function formatAnalysisSummary(analysis: SmartRoutingAnalysis): string {
  const { complexity, privacy } = analysis;

  let summary = "";

  // ë³µì¡ë„ ì •ë³´
  const complexityEmoji =
    complexity.level === "simple" ? "ğŸŸ¢" :
    complexity.level === "general" ? "ğŸŸ¡" :
    complexity.level === "complex" ? "ğŸŸ " : "ğŸ”´";

  summary += `${complexityEmoji} ë³µì¡ë„: ${complexity.score}/5 (${complexity.reason})\n`;

  // í”„ë¼ì´ë²„ì‹œ ì •ë³´
  if (privacy.isPrivate) {
    const privacyEmoji = privacy.level === "critical" ? "ğŸ”´" : privacy.level === "sensitive" ? "ğŸŸ " : "ğŸŸ¡";
    summary += `${privacyEmoji} ë¯¼ê°ë„: ${privacy.reason}\n`;
  }

  // ì¶”ì²œ ëª¨ë¸
  const tierLabels: Record<SuggestedModelTier, string> = {
    free: "ğŸ†“ ë¬´ë£Œ ëª¨ë¸",
    cheap: "ğŸ’° ì €ë ´í•œ ëª¨ë¸",
    premium: "ğŸ§  ê³ ê¸‰ ëª¨ë¸",
    local: "ğŸ”’ ë¡œì»¬ ì²˜ë¦¬",
  };
  summary += `ğŸ“ ì¶”ì²œ: ${tierLabels[analysis.suggestedTier]}`;

  return summary;
}
