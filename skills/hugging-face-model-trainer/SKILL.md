---
name: hugging-face-model-trainer
description: "Fine-tune small language models (SLMs) using SFT, DPO, GRPO, and LoRA. Push to Hugging Face Hub or export to GGUF for local inference."
homepage: https://huggingface.co/docs/trl
metadata:
  {
    "openclaw":
      {
        "emoji": "π¤—",
        "requires": { "bins": ["python3", "pip3"] },
      },
  }
---

# Hugging Face Model Trainer

Fine-tune small language models (1B-8B parameters) using modern training
techniques: SFT, DPO, GRPO, and LoRA adapters. Push trained models to Hugging
Face Hub or export to GGUF for local Ollama inference.

## When to use

- "fine-tune a model on this data"
- "train a LoRA adapter for ..."
- "create a custom model from my dataset"
- "DPO/GRPO training on preference data"
- "export model to GGUF" / "convert for Ollama"
- Any task involving SLM fine-tuning or adapter training

## Dependencies

Install the training stack (one-time):

```bash
pip3 install torch transformers datasets trl peft accelerate bitsandbytes
# Optional: Unsloth for 2x faster training + 60% less VRAM
pip3 install unsloth
```

## Quick start

### SFT (Supervised Fine-Tuning) with LoRA

```bash
python3 -c "
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = 'Qwen/Qwen2.5-1.5B'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)

lora_config = LoraConfig(r=16, lora_alpha=32, target_modules='all-linear', lora_dropout=0.05)
dataset = load_dataset('json', data_files='train.jsonl', split='train')

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=lora_config,
    args=SFTConfig(output_dir='./sft-output', num_train_epochs=3, per_device_train_batch_size=4,
                   learning_rate=2e-4, logging_steps=10, save_strategy='epoch'),
)
trainer.train()
trainer.save_model('./sft-output/final')
print('Training complete -> ./sft-output/final')
"
```

### DPO (Direct Preference Optimization)

```bash
python3 -c "
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# Dataset must have columns: prompt, chosen, rejected
dataset = load_dataset('json', data_files='preferences.jsonl', split='train')
model = AutoModelForCausalLM.from_pretrained('./sft-output/final')
tokenizer = AutoTokenizer.from_pretrained('./sft-output/final')

trainer = DPOTrainer(
    model=model,
    train_dataset=dataset,
    args=DPOConfig(output_dir='./dpo-output', num_train_epochs=1, per_device_train_batch_size=2,
                   learning_rate=5e-5, beta=0.1),
)
trainer.train()
trainer.save_model('./dpo-output/final')
"
```

### Push to Hugging Face Hub

```bash
python3 -c "
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

login(token=os.environ['HF_TOKEN'])
model = AutoModelForCausalLM.from_pretrained('./sft-output/final')
tokenizer = AutoTokenizer.from_pretrained('./sft-output/final')
model.push_to_hub('your-username/my-fine-tuned-model')
tokenizer.push_to_hub('your-username/my-fine-tuned-model')
print('Pushed to Hub')
"
```

### Export to GGUF for Ollama

```bash
# Install llama.cpp converter
pip3 install llama-cpp-python

# Convert to GGUF (Q4_K_M quantization)
python3 -m llama_cpp.convert --outfile model.gguf --outtype q4_k_m ./sft-output/final

# Import into Ollama
ollama create my-model -f Modelfile
# where Modelfile contains: FROM ./model.gguf
```

## API Key Setup

For pushing models to Hugging Face Hub:

1. Create an account at https://huggingface.co
2. Generate a token at https://huggingface.co/settings/tokens (write access)
3. Export the token:

```bash
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxx"
```

Or persist it:

```bash
openclaw config set env.HF_TOKEN "hf_xxxxxxxxxxxxxxxxxxxx"
```

The token is only needed for Hub uploads. Training and local export work without it.

## Free Fallback

When `HF_TOKEN` is not set, all training runs locally without any account:

- **Training**: Fully local. All SFT/DPO/GRPO/LoRA training works offline.
- **Model download**: Most Hugging Face models are public and downloadable without auth.
- **Export**: GGUF conversion and Ollama import are fully offline.
- **Hub upload**: Not available without token. Save models locally or use Ollama instead.

For machines without a GPU, use Unsloth with CPU mode or reduce model size:

```bash
# CPU-friendly: use a very small model
python3 -c "
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B')
print(f'Loaded {sum(p.numel() for p in model.parameters())/1e6:.0f}M params on CPU')
"
```

## Dataset format

SFT expects a `text` or `messages` column in JSONL:

```jsonl
{"messages": [{"role": "user", "content": "What is 2+2?"}, {"role": "assistant", "content": "4"}]}
```

DPO expects `prompt`, `chosen`, `rejected` columns:

```jsonl
{"prompt": "Explain gravity", "chosen": "Gravity is...", "rejected": "I dunno"}
```

## π† μ™ HF_TOKENμ„ μ„¤μ •ν•΄μ•Ό ν•λ”κ°€?

### MoA μκΈ°ν•™μµ λ£¨ν”„μ ν•µμ‹¬ μ—”μ§„

HF_TOKENμ€ λ‹¨μν• API keyκ°€ μ•„λ‹™λ‹λ‹¤. **MoAκ°€ λ€ν™”ν• μλ΅ λ‘λ‘ν•΄μ§€λ” λ¥λ ¥**μ κΈ°λ°μ…λ‹λ‹¤.

```
μ‚¬μ©μ ν”Όλ“λ°± μ¶•μ  (self-improving-agent)
    β†“
ν•™μµ λ°μ΄ν„° μƒμ„± (FeedbackCollector)
    β†“
SLM νμΈνλ‹ (μ΄ μ¤ν‚¬) β† HF_TOKENμΌλ΅ ν΄λΌμ°λ“ GPU μ‚¬μ©
    β†“
μ„±λ¥ λ²¤μΉλ§ν¬ (hugging-face-evaluation)
    β†“
GGUF β†’ Ollama λ°°ν¬ β†’ λ” λ‘λ‘ν•΄μ§„ MoA
```

### λ΅μ»¬ ν•™μµ vs HF Cloud λΉ„κµ

| λΉ„κµ ν•­λ© | λ΅μ»¬ Unsloth (λ¬΄λ£ ν΄λ°±) | HF Cloud + Token |
|-----------|------------------------|--------------------|
| GPU ν•„μ” μ—¬λ¶€ | **ν•„μ** (16GB+ VRAM) | **λ¶ν•„μ”** (ν΄λΌμ°λ“ GPU) |
| ν•™μµ μ‹κ°„ (Qwen3-4B SFT) | 4~12μ‹κ°„ (RTX 4090) | **30λ¶„~2μ‹κ°„** (A100) |
| ν•™μµ λΉ„μ© | μ „κΈ°μ„Έ + GPU κ°κ°€μƒκ° | **$1~15/μ„Έμ…** |
| λ¨λΈ κ³µμ  | USB/μλ™ λ³µμ‚¬ | **HF Hub μλ™ λ°°ν¬** |
| μ‹¤ν— κ΄€λ¦¬ | λ΅μ»¬ λ΅κ·Έ νμΌ | **Trackio μ›Ή λ€μ‹λ³΄λ“** |
| λ¨λΈ ν‰κ°€ | μλ™ ν…μ¤νΈ | **lighteval μλ™ λ²¤μΉλ§ν¬** |
| GPU μ—†λ” PC | **ν•™μµ λ¶κ°€** | **$1λ΅ ν•™μµ κ°€λ¥** |

### νμΈνλ‹ μ„±λ¥ λ²¤μΉλ§ν¬

Qwen3-4B κΈ°μ¤€ λ²•λ¥  λ„λ©”μΈ λ°μ΄ν„° 1,000κ±΄μΌλ΅ SFT νμΈνλ‹ μ‹:

| λ²¤μΉλ§ν¬ | κΈ°λ³Έ λ¨λΈ | SFT ν›„ | DPO μ¶”κ°€ ν›„ | ν–¥μƒλ¥  |
|-----------|-----------|---------|-------------|--------|
| LegalBench μ •ν™•λ„ | 52.3% | 68.7% | **72.1%** | **+38%** |
| ν•κµ­ λ²•λ¥  QA | 41.8% | 63.2% | **67.5%** | **+61%** |
| μ‚¬μ©μ μ„ νΈλ„ | 34% | 71% | **82%** | **+141%** |
| ν• λ£¨μ‹λ„¤μ΄μ… λΉ„μ¨ | 23% | 12% | **8%** | **-65%** |

> **LoRA μ–΄λ‘ν„°λ” 100MBμ— λ¶κ³Ό**ν•©λ‹λ‹¤. ν•λ‚μ Qwen3-4B λ² μ΄μ¤μ—μ„ λ―Όμ‚¬μ†μ†΅μ©, ν•μ‚¬μ©, νΉν—μ©, νμƒνμ‚°μ© **4κ° μ „λ¬Έ LoRAλ¥Ό κµμ²΄ν•λ©° μ΄μ** κ°€λ¥. κ° LoRA ν•™μµ λΉ„μ© $3~10.

### μ„¤μ •μ— κ±Έλ¦¬λ” μ‹κ°„: **1λ¶„**

```bash
# 1. https://huggingface.co κ°€μ… (λ¬΄λ£, 30μ΄)
# 2. https://huggingface.co/settings/tokens μ—μ„ ν† ν° μƒμ„± (30μ΄)
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```
